---
title: "Internal Accuracy Assessment of Machine-learning Predictive Ecosystem Maps"
subtitle: "by Will MacKenzie & Kiri Daust"
Methods author: "Will MacKenzie"
Script author: "Gen Perkins; Kiri Daust; Will MacKenzie; Colin Chisholm; Matt Coghill"
date: "8/06/2019"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r global_options, include=FALSE }
require(knitr)

```

```{r setup, include=FALSE}

library(data.table)
library(scales)
#library(caret)
library(sf)
library(ranger)
library(tidyverse)
library(fasterize)
library(stringr)
library(dplyr)
library(raster)
library(readxl)
library(foreach)
library(tidymodels)
library(themis)
library(vip)
library(stringi)
library(R.utils)
library(colorspace)
library(janitor)

#devtools::install_github("tidymodels/tune")
#devtools::install_github("tidymodels/parsnip")

```

# Introduction

This script uses training point samples collected from Stage 1 transects and other point data collected.

This is by boot strapping the transects where 90% are use to provide training points to the build a map, and 10% are withheld and used as AA transects. 
This analytical approach is then used to guide improvements to the map.

1. Load data prepared in [04a script](https://github.com/bcgov-c/BEC_DevExchange_Work/blob/master/04a_TrainingPt_TransectImport%26Clean.Rmd). 
2. Split dataset into training and testing sets
3. Run the model and record model performance metrics
4. Best model is selected to create a map and associated entropy layer 


Prepare directories

```{r session setup, tidy = TRUE, warning=FALSE}

AOI <- "Deception"
#AOI <- "BoundaryTSA"
AOI <- "DateCreek"
AOI <- "EagleHills"

# set up file structure
AOI_dir <- file.path(".", paste0(AOI,"_AOI"))
cov_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")
shapes_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers")
input_pnts_dir <- file.path(AOI_dir, "1_map_inputs", "trainingData")
out_dir <- file.path(AOI_dir, "3_maps_analysis","models")

# read in temp functions
source(here::here('_functions', 'model_gen_tidy.R'))
source(here::here('_functions', 'model_gen_tidy_nf.R'))
source(here::here('_functions', 'acc_metrix.R'))
source(here::here('_functions', 'doc_theme_pem.R'))
source(here::here('_functions', 'balance_recipe.R'))

# read in map and model keys
map.key  <- read.csv(file.path(AOI_dir, "_MapUnitLegend", 
                                 paste0(AOI, "_MapUnitLegend.csv")), 
                       stringsAsFactor = FALSE)

fMat <- read.csv(file.path(AOI_dir, "_MapUnitLegend", 
                                  "fuzzy_matrix_basic.csv")) %>%
  dplyr::select(c(target, Pred, fVal))
  
  
fMat <- data.table(fMat)
# read in model parameters 
model_param <- file.path(AOI_dir, "_MapUnitLegend", "models.xlsx")

# set up model parameters:  
mparam <- read_xlsx(model_param, "models") %>% filter(to_run == 1)
map_res <- mparam$resolution
data_res <- paste0("att_", map_res, "m")
mname <- paste0(mparam$model_name)
mrep <- mparam$model_rep

# check which catergory of model to be produced
mtype <- case_when(
  str_detect(mname, "for_nf")  ~ "forest_non_forest",
  str_detect(mname, "nf_") ~ "non_forest",
  str_detect(mname, "fore") ~ "forest"
)

# get covariates
mcov <- read_xlsx(model_param, "covariates", skip = 2) %>%
  filter(!!sym(mparam$covariates) == 1) %>%
  dplyr::select(covariate)

# get training point sets
mtpt <- read_xlsx(model_param, "training_pts", skip = 2) %>%
  filter(!!sym(mparam$training_pts) == 1) %>%
  dplyr::select(tp_code)%>%
  pull

# get the map unit level 
mmu <- read_xlsx(model_param, "map_unit", skip = 2) %>%
   filter(!!sym(mparam$map_unit) == 1) %>%
  dplyr::select(legend_column)%>%
  pull

mmu <- case_when(
  mmu == "column_mu" ~ "MapUnit", 
  mmu == "column_ss" ~ "SiteSeries",
  mmu == "column_ass" ~ "Association",
  mmu == "column_cls" ~ "Class",
  mmu == "column_grp" ~ "Group",
  mmu == "column_typ" ~ "Type"
)

# set up outfolder: 

if(!dir.exists(file.path(out_dir, mtype))){dir.create(file.path(out_dir, mtype))} 

out_dir <- file.path(out_dir, mtype, mname, mrep) 

# set up model folder: 
if(!dir.exists(out_dir)){
  dir.create(out_dir)} else { 
    print ("folder already exists for this model name - are you sure you want to overwrite the results?")}


# read in the all points dataset to enable map accuracy assessment (normally s1 or s2 points )
# 
# if(AOI == "BoundaryTSA"){
# all_points_predicted <- st_read(file.path(input_pnts_dir, data_res, "s1_transect_all_pts_att.gpkg")) %>%
#   mutate(tid = gsub("_[[:alpha:]].*","", id)) %>%
#   dplyr::select(-c('x','y', 'id')) 
# } else {
# 
# all_points_predicted <- st_read(file.path(input_pnts_dir, data_res, "s1_transect_all_pts_att.gpkg")) %>%
#   mutate(tid = gsub("_[[:alpha:]].*","",transect_id)) 
# }


```


##Create raster stack of spatial layers

```{r create raster stack of spatial variables}

res_folder <- paste0(map_res, "m") # or for trim testing use line below
#res_folder <- paste0(map_res, "m_trim")

rast_list <- list.files(file.path(cov_dir, res_folder), pattern = ".tif$", full.names = TRUE)

# filter based on covariate for model param
rast_list <- rast_list[tolower(gsub(".tif", "", basename(rast_list))) %in% tolower(mcov$covariate)]

mcols <- gsub(".tif","", tolower(basename(rast_list)))

bec_shp <- st_read(file.path(shapes_dir, "bec.gpkg"), quiet = TRUE)

```


##Prepare training and testing datasets for the model

Using the point data generated in various forms from the 04a script, iteratively read in those points and create and save models. Model data is saved in a spreadsheet so that the spreadsheet can be filtered later to find the model with the best accuracy metrics. If the model is broken down by subzone and you have lots of different input datasets, this could take a while!

The random forest model is created using the training data set from above. Repeated cross validation is used to generate model metrics, though remember this is using the training set. We will evaluate our own metrics using the held back test set that was created above.

The model is able to be fine tuned quite nicely. Certain preprocessing parameters are included here to limit the amount of data that is used in the final model, including a correlation cutoff (remove covariates that are highly correlated), nero-zero variance detection (removes covariates that have 0 or close to 0 variance), centering and scaling of the data, and more, though these are the more common setup parameters. Having this prevents actual preprocessing to occur and allows for much more efficient processing.

Using the test dataset, the model is evaluated using the held back points. A confusion matrix is generated and then saved to the model output folder.


```{r read in datasets }
# read in sets of data 


if(length(mtpt) == 1){

  indata <- list.files(file.path(input_pnts_dir, data_res), paste0(mtpt,"_att.*.gpkg$"), full.names = TRUE)
  
  # for trim testing only 
  # indata <- list.files(file.path(input_pnts_dir, "att_5m_trim"),full.names = TRUE)
  ## remove above line for non trim testing  

  tpts <- st_read(indata[1]) 
  
  infiles <- basename(indata) 
  
} else { 
   # get list of input training points for report
   infiles <- mtpt
        
   # read in first file 
   fileoi <- mtpt[1]
   indata <- list.files(file.path(input_pnts_dir, data_res), paste0(fileoi,"_att.*.gpkg$"), full.names = TRUE)
   tpts <- st_read(indata, quiet = TRUE) 
   
   cols_to_keep <- names(tpts)
   otherfiles <- mtpt[-1]
   
   for (filei in otherfiles ){
      #filei <- otherfiles[3]
     
     ind <- list.files(file.path(input_pnts_dir, data_res), paste0(filei,"_att.*.gpkg$"), full.names = TRUE)
     if(length(ind>1)) {
       ind <- ind[grep(paste0("^",filei,"_att.gpkg$"), basename(ind))]
     }
      file_read <- st_read(ind, quiet = TRUE) 
      file_names <- names(file_read)
      
      file_read <- file_read[,file_names %in% cols_to_keep]
      
      tpts <- bind_rows(tpts, file_read) 
   }
 }   
  
#  MU_count <- tpts %>% dplyr::count(mapunit1)
table(tpts$mapunit1)

# match to the key and filter for forest and non_forest points

subzones <- unique(bec_shp$BGC_LABEL)
subzones <- gsub("\\s+","",subzones )

#aa <- tpts

tpts  <- tpts %>%
  cbind(st_coordinates(.)) %>%
  mutate(fnf = ifelse(grepl(paste0(subzones, collapse = "|"), mapunit1), "forest", "non_forest")) %>%
  st_join(st_transform(bec_shp[, "BGC_LABEL"], st_crs(.)), join = st_nearest_feature) %>%
  st_drop_geometry() %>% 
  dplyr::select(fnf, everything()) %>% 
  dplyr::rename(bgc_cat = BGC_LABEL) %>% 
  rename_all(.funs = tolower) %>% 
  droplevels()

#write_csv(tpts, "s1_clean_pts_all_fnf.csv")


# match the column for map unit based on key 
# select the target column using the mapkey if needed: 
map.key.sub <- map.key %>%
      dplyr::select(BaseMapUnit, !!sym(mmu)) %>%
      distinct()
  
  tpts <- tpts %>% left_join(map.key.sub, by = c("mapunit1" = "BaseMapUnit")) %>%
    left_join(map.key.sub, by = c("mapunit2" = "BaseMapUnit")) %>%
    dplyr::select(-mapunit1, -mapunit2) %>%
    dplyr::rename("mapunit1" = MapUnit.x,
                  "mapunit2" = MapUnit.y) %>%
    dplyr::select(mapunit1, mapunit2, everything())

  
# filter for forest or non_forest points as required
if(mtype %in% c("forest", "non_forest")) {
   tpts <- tpts %>% filter(fnf == mtype)
} 


tpts <- tpts %>%
    dplyr::mutate(target = as.factor(mapunit1),
                          target2 = as.factor(mapunit2))

# filter columns
mpts <- tpts %>%
     dplyr::select(target, target2, transect_id, tid, slice, bgc_cat, any_of(mcols)) 

#if (aoi == "eagleHIlls)
#mpts <- mpts %>%
#  mutate(slice = 1)
  
# OPTION: filter groups less than 20 pts
MU_count <- mpts %>% dplyr::count(target) %>% filter(n > 20) 
  
mpts <- mpts %>% #filter(target %in% MU_count$target)  %>%
    droplevels() 
  
mpts <- mpts %>% 
  dplyr::mutate(bgc_cat = gsub("\\s+","", bgc_cat))

#write_csv(mpts, "s1_clean_pts_all_fnf.csv")

```

Run the forest models - either for all units or per BGC. 

```{r run model}
# # if the name contains BGC then run models per BGC else run all
# if (str_detect(mname, 'bgc')) {
    
  zones <- c(as.character(subzones))
  
  bgc_pts_subzone <- lapply(zones, function (i){
      #i <- zones[11]
      pts_subzone <- mpts %>%
        filter(str_detect(target, as.character(paste0(i, "_")))) %>%
        #drop_na() %>%
        droplevels()
      
      if(nrow(pts_subzone) == 0){ pts_subzone = NULL} else {ppts_subzone = pts_subzone }
      pts_subzone
  })
  
  # generate a name for list objects removing NULL values
  names <- unlist(lapply(bgc_pts_subzone, is.null))
  
  if(AOI == "EagleHills"){
      
    names(bgc_pts_subzone) <- zones
    
  } else {
  
  zone_names <- zones[-(which(ll <- names)) ] 
  
  # remove null or missing subzones data sets 
  bgc_pts_subzone =  bgc_pts_subzone[-which(sapply(bgc_pts_subzone, is.null))]
  names(bgc_pts_subzone) <- zone_names
  
  } 
  ## run model with all variables
  
  # Boundary()
#  [1] "ICHmk1"  "IDFdm1"  "ICHxw"   "ESSFdc1" "ESSFdc2" "MSdm1"   "IDFdh"  
#  [8] "ICHdw1"  "ESSFmh"  "ICHmw5" 
  
  # need to redo
 ## "ESSFdc1" [4]
 ## "ESSFdc2" [5]
 ##  "IDFdh"  [7]
  
  model_bgc <- lapply(names(bgc_pts_subzone), function(xx) {
   

    xx <- names(bgc_pts_subzone[2])
    
    inmdata = bgc_pts_subzone[[xx]]
    out_name = names(bgc_pts_subzone[xx])
    
    # if(any(names(inmdata) %in% "x")) {
    # 
    # inmdata_all <- inmdata %>%
    #   dplyr::select(-c(x, y))
    # } else {
    #  inmdata_all <- inmdata
    # }
    inmdata_all <- inmdata
    #table(inmdata_all$target)
    #length(inmdata_all$target)
    
    outDir = file.path(paste(out_dir, out_name, sep = "/"))
   
       # test line for mapping    
    #inmdata_all <- inmdata_all %>%
    #  filter(slice != "1" )
    
    MU_count <- inmdata_all %>% dplyr::count(target) %>% filter(n >10) # use 10 for smo

    inmdata_all <-  inmdata_all %>% filter(target %in% MU_count$target)  %>%
       droplevels() #%>%
       #dplyr::select(-bgc_cat)
 
    # SBSmc2
   # ds_ratio = 15 #80#70 #100  #(0 - 100, Null = 100)
  #  sm_ratio = 0.5#0.2#0.3 #0.1 #0 - 1, 1 = complete smote 
            
            # ds_15_sm_0.5 : mapunit (spatial)
            # ds 70_sm_0.3 : ovreall (aspatial)
    
    # ESSFmc 
    ds_ratio = 15  #15   #40  #(0 - 100, Null = 100)
    sm_ratio = 0.2 #0.2 #0.7 #0 - 1, 1 = complete smote
  
                # ds_15_sm_0.2 : mapunit (spatial)
                # ds 90_sm_0.9 : ovreall (aspatial)
    
      # ESSFmcw
     # ds_ratio = 40 #15 #40
    #  sm_ratio = 0.8 #0.6 #0.8 #0.9
      
                # ds_20 : mapunit (spatial)
                # ds 29_sm_0.9 : ovreall (aspatial)
   # ds_ratio = NA
   # sm_ratio = NA
    
#    #  forest non-forest model 
#    model_gen_tidy_nf(trDat = inmdata_all,
#              target = "target",
#              target2 = "target2",
#              outDir = outDir,
#              mname = mname,
#              infiles = infiles,
#              mmu = mmu)

    # for Date Creek(ICHmc1 contains one site from ICHmc2 - remove 7 points)
    if(xx == "ICHmc1"){
      inmdata_all <- inmdata_all %>%
        dplyr::filter(slice != "2")
    }
 
    
    # or full model 
    model_gen_tidy(trDat = inmdata_all,
              target = "target",
              target2 = "target2",
              tid = "tid",
              ds_ratio = ds_ratio , # NA if no downsample
              sm_ratio = sm_ratio, # NA if no smoting 
              outDir = outDir,
              mname = mname,
              rseed = 456,
              infiles = infiles,
              mmu = mmu, 
              field_transect = NA)

     }) 
 
 #}  
  
```


# Run the entire study area (combined BGCs with BGC as covar)
Note this is not iterated by slice but tid? or none? Unsure yet?


```{r}
  # filter for > 20 points 
  MU_count <- mpts %>% dplyr::count(target) %>% filter(n > 20) 
  mpts <- mpts %>% filter(target %in% MU_count$target)  %>%
    filter(target != "") %>%
    droplevels() 
  
   outname = "all"
   outDir = file.path(paste(out_dir, outname, sep = "/"))


  # or full model 
  model_gen_tidy(trDat = mpts,
              target = "target",
              target2 = "target2",
              tid = "tid",
              ds_ratio = NA, #ds_ratio , # NA if no downsample
              sm_ratio = NA, #sm_ratio, # NA if no smoting 
              outDir = outDir,
              mname = mname,
              rseed = 456,
              infiles = infiles,
              mmu = mmu, 
              field_transect = NA)

  


```


 
Run the non_forest models 


```{r}

if(str_detect(mname, 'nf_')) {

  # filter for > 20 points 
  MU_count <- mpts %>% dplyr::count(target) %>% filter(n > 20) 
  mpts <- mpts %>% filter(target %in% MU_count$target)  %>%
    filter(target != "") %>%
    droplevels() 
  
  # check covars
  covars <- mpts %>% dplyr::select(-c("target" )) 
  covars <- covars[complete.cases(covars[ ,1:length(covars)]),]
 
  ###############################################################

# run the function using tidy models

  model_gen_tidy_nf(trDat = mpts,
              target = "target",
              outDir = out_dir,
              mname = mname,
              infiles = infiles,
              mmu = mmu)
            
#   
} 

```


 