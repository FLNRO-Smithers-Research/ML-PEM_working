---
title: "Internal Accuracy Assessment of Machine-learning Predictive Ecosystem Maps : Non-Forest model"
date: "13/01/2022"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r global_options, include=FALSE }
require(knitr)

```

```{r setup, include=FALSE}

library(data.table)
library(scales)
library(caret)
library(sf)
library(ranger)
library(tidyverse)
library(fasterize)
library(stringr)
library(dplyr)
library(raster)
library(readxl)
library(foreach)
library(tidymodels)
library(themis)
library(vip)
library(stringi)
library(R.utils)
library(colorspace)
library(janitor)

#devtools::install_github("tidymodels/tune")
#devtools::install_github("tidymodels/parsnip")

```

# Introduction

This script variety of points to build a non-forest model accross the entire study area. 

In this process we have a simple approach of cross validation. 

```{r session setup, tidy = TRUE, warning=FALSE}

AOI <- "Deception"

# set up file structure
AOI_dir <- file.path(".", paste0(AOI,"_AOI"))
#AOI_dir <- file.path(paste0(AOI, "_AOI"))
cov_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")
shapes_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers")
input_pnts_dir <- file.path(AOI_dir, "1_map_inputs", "trainingData")
out_dir <- file.path(AOI_dir, "3_maps_analysis","models")

# read in temp functions
source(here::here('_functions', 'model_gen_tidy_nf.R'))
source(here::here('_functions', 'acc_metrix.R'))
source(here::here('_functions', 'doc_theme_pem.R'))
source(here::here('_functions', 'balance_recipe.R'))

# read in map and model keys
map.key  <- read.csv(file.path(AOI_dir, "_MapUnitLegend", 
                                 paste0(AOI, "_MapUnitLegend.csv")), 
                       stringsAsFactor = FALSE)

fMat <- read.csv(file.path(AOI_dir, "_MapUnitLegend", 
                                  "fuzzy_matrix_basic.csv")) %>%
  dplyr::select(c(target, Pred, fVal))
  
  
 fMat <- data.table(fMat)
# read in model parameters 
model_param <- file.path(AOI_dir, "_MapUnitLegend", "models.xlsx")

# set up model parameters:  
mparam <- read_xlsx(model_param, "models") %>% filter(to_run == 1)
map_res <- mparam$resolution
data_res <- paste0("att_", map_res, "m")
mname <- paste0(mparam$model_name)
mrep <- mparam$model_rep


# check which catergory of model to be produced
mtype <- case_when(
  str_detect(mname, "for_nf")  ~ "forest_non_forest",
  str_detect(mname, "nf_") ~ "non_forest",
  str_detect(mname, "fore") ~ "forest"
)

# get covariates
mcov <- read_xlsx(model_param, "covariates", skip = 2) %>%
  filter(!!sym(mparam$covariates) == 1) %>%
  dplyr::select(covariate)

# get training point sets
mtpt <- read_xlsx(model_param, "training_pts", skip = 2) %>%
  filter(!!sym(mparam$training_pts) == 1) %>%
  dplyr::select(tp_code)%>%
  pull

# get the map unit level 
mmu <- read_xlsx(model_param, "map_unit", skip = 2) %>%
   filter(!!sym(mparam$map_unit) == 1) %>%
  dplyr::select(legend_column)%>%
  pull

mmu <- case_when(
  mmu == "column_mu" ~ "MapUnit", 
  mmu == "column_ss" ~ "SiteSeries",
  mmu == "column_ass" ~ "Association",
  mmu == "column_cls" ~ "Class",
  mmu == "column_grp" ~ "Group",
  mmu == "column_typ" ~ "Type"
)

# set up outfolder: 

if(!dir.exists(file.path(out_dir, mtype))){dir.create(file.path(out_dir, mtype))} 

out_dir <- file.path(out_dir, mtype, mname, mrep) 

# set up model folder: 
if(!dir.exists(out_dir)){
  dir.create(out_dir)} else { 
    print ("folder already exists for this model name - are you sure you want to overwrite the results?")}


```


##Create raster stack of spatial layers

```{r create raster stack of spatial variables}

res_folder <- paste0(map_res, "m") # or for trim testing use line below
#res_folder <- paste0(map_res, "m_trim")

rast_list <- list.files(file.path(cov_dir, res_folder), pattern = ".tif$", full.names = TRUE)

# filter based on covariate for model param
rast_list <- rast_list[tolower(gsub(".tif", "", basename(rast_list))) %in% tolower(mcov$covariate)]

mcols <- gsub(".tif","", tolower(basename(rast_list)))

bec_shp <- st_read(file.path(shapes_dir, "bec.gpkg"), quiet = TRUE)


```


##Prepare training and testing datasets for the model

Using the point data generated in various forms from the 04a script, iteratively read in those points and create and save models. Model data is saved in a spreadsheet so that the spreadsheet can be filtered later to find the model with the best accuracy metrics. If the model is broken down by subzone and you have lots of different input datasets, this could take a while!

The random forest model is created using the training data set from above. Repeated cross validation is used to generate model metrics, though remember this is using the training set. We will evaluate our own metrics using the held back test set that was created above.

The model is able to be fine tuned quite nicely. Certain preprocessing parameters are included here to limit the amount of data that is used in the final model, including a correlation cutoff (remove covariates that are highly correlated), nero-zero variance detection (removes covariates that have 0 or close to 0 variance), centering and scaling of the data, and more, though these are the more common setup parameters. Having this prevents actual preprocessing to occur and allows for much more efficient processing.

Using the test dataset, the model is evaluated using the held back points. A confusion matrix is generated and then saved to the model output folder.


```{r read in datasets }
# read in sets of data 

if(length(mtpt) == 1){

  indata <- list.files(file.path(input_pnts_dir, data_res), paste0(mtpt,"_att.*.gpkg$"), full.names = TRUE)
  
  # for trim testing only 
  # indata <- list.files(file.path(input_pnts_dir, "att_5m_trim"),full.names = TRUE)
  ## remove above line for non trim testing  

  tpts <- st_read(indata[1]) 
  
  infiles <- basename(indata) 
  
} else { 
   # get list of input training points for report
   infiles <- mtpt
        
   # read in first file 
   fileoi <- mtpt[1]
   indata <- list.files(file.path(input_pnts_dir, data_res), paste0(fileoi,"_att.*.gpkg$"), full.names = TRUE)
   tpts <- st_read(indata, quiet = TRUE) 
   
   cols_to_keep <- names(tpts)
   otherfiles <- mtpt[-1]
   
   for (filei in otherfiles ){
      #filei <- otherfiles[3]
     print(filei)
     ind <- list.files(file.path(input_pnts_dir, data_res), paste0(filei,"_att.*.gpkg$"), full.names = TRUE)
     if(length(ind>1)) {
       ind <- ind[grep(paste0("^",filei,"_att.gpkg$"), basename(ind))]
     }
      file_read <- st_read(ind, quiet = TRUE) 
      file_names <- names(file_read)
      
      file_read <- file_read[,file_names %in% cols_to_keep]
      
      tpts <- bind_rows(tpts, file_read) 
   }
 }   
  
#  MU_count <- tpts %>% dplyr::count(mapunit1)
table(tpts$mapunit1)

# 
# ws_check = tpts %>% dplyr::select(c("mapunit1", "mapunit2", 
#                                   "observer" , "comments")) %>%
#   distinct()
# 
# ws1 <- ws_check %>% filter(mapunit1 %in% c( "Ws","Ws03","Ws04","Ws08", "Wun")) 
# ws2 <- ws_check %>% filter(mapunit2 %in% c( "Ws","Ws03","Ws04","Ws08", "Wun"))      
# 
# ws_out <- rbind(ws1, ws2)
# write.csv(ws_out, "ws_check_deception.csv")


# match to the key and filter for forest and non_forest points
subzones <- unique(bec_shp$BGC_LABEL)
subzones <- gsub("\\s+","",subzones )

tpts  <- tpts %>%
  cbind(st_coordinates(.)) %>%
#  mutate(fnf = ifelse(grepl(paste0(subzones, collapse = "|"), mapunit1), "forest", "non_forest")) %>%
  st_join(st_transform(bec_shp[, "BGC_LABEL"], st_crs(.)), join = st_nearest_feature) %>%
  st_drop_geometry() %>% 
#  dplyr::select(fnf, everything()) %>% 
  dplyr::rename(bgc_cat = BGC_LABEL) %>% 
  rename_all(.funs = tolower) %>% 
  droplevels()

# match the column for map unit based on key 
# select the target column using the mapkey if needed: 
#map.key.sub <- map.key %>%
#      dplyr::select(BaseMapUnit, !!sym(mmu), ForNonfor) %>%
#      distinct()

map.key.sub <- map.key %>%
      dplyr::select(BaseMapUnit, !!sym(mmu), Type3) %>%
      distinct() %>%
      dplyr::rename("ForNonfor" = Type3)


tpts1 <- tpts %>% left_join(map.key.sub, by = c("mapunit1" = "BaseMapUnit")) %>%
    left_join(map.key.sub, by = c("mapunit2" = "BaseMapUnit")) %>%
    dplyr::select(-mapunit1, -mapunit2) %>%
    dplyr::rename("mapunit1" = MapUnit.x,
                  "mapunit2" = MapUnit.y,
                  "fnf1" = ForNonfor.x,
                  "fnf2" = ForNonfor.y) %>%
    dplyr::select(mapunit1, mapunit2, fnf1, fnf2, everything())


# temp chcek 
#xx <- tpts1 %>%
#  dplyr::select(mapunit1, mapunit2, fnf1,fnf2) %>%
#  distinct() %>%
#  filter(fnf1 == "Nfor")
 # filter(fnf1 == "Nfor"|fnf2 == "Nfor")

# check the distribution
table(tpts1$fnf1)

unique(tpts1$fnf1)
tpts1 <- tpts1 %>% 
  filter(!fnf1 == "") %>%
  filter(!is.na(fnf1))


# filter for forest or non_forest points as required
for_tpts <- tpts1 %>% filter(fnf1 == "For") # select forest pts 
#length(for_tpts$mapunit1)
tpts <- tpts1 %>% filter(!fnf1 == "For") %>%
  dplyr::select(-mapunit1, -mapunit2)%>%
  dplyr::rename("mapunit1" = fnf1,
         "mapunit2" = fnf2)


sort(table(tpts1$mapunit1))

# maximum no of pts = 600 so will select 600 random forest points 

ftpts <- sample(1:length(for_tpts$mapunit1), 2100)
fort <- for_tpts[ftpts,]
fort <- fort %>%
  mutate(mapunit1 = "For")%>%
  dplyr::select(-mapunit2) %>%
  dplyr::rename("mapunit2" = fnf2) %>%
  dplyr::select( -fnf1)


tpts <- bind_rows(tpts, fort)

#write_csv(tpts, "s1_clean_pts_all_fnf.csv")
tpts <- tpts %>%
    dplyr::mutate(target = as.factor(mapunit1),
                          target2 = as.factor(mapunit2))

# filter columns
mpts <- tpts %>%
     dplyr::select(target, target2, tid, bgc_cat, any_of(mcols))


# OPTION: filter groups less than 20 pts (across all bgcs)
MU_count <- mpts %>% dplyr::count(target) %>% filter(n > 20) 
  
mpts <- mpts %>% filter(target %in% MU_count$target)  %>%
    filter(target != "") %>%
    droplevels() 

mpts <- mpts %>% 
  dplyr::filter(bgc_cat %in% c("ESSFmc","ESSFmcw","SBS mc 2"))%>%
    droplevels() 
                
 #"ESSFmcp"  "SBS dk"  


```

Run the non_forest models over the entire area

```{r}

#covars <- mpts %>% dplyr::select(-c("target" )) 
#covars <- covars[complete.cases(covars[ ,1:length(covars)]),]
 
  model_gen_tidy_nf(trDat = mpts,
              target = "target",
              outDir = out_dir,
              mname = mname,
              infiles = infiles,
              mmu = mmu)
            
```


 