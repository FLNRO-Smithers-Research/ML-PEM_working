---
title: "Stage 1 Sample Design for Training Point Generation and Internal Accuracy Assessment of Machine-learning Predictive Ecosystem Maps"
subtitle: "by Will MacKenzie & Kiri Daust"
Methods author: "Will MacKenzie"
Script authors: "Kiri Daust; Will MacKenzie; Colin Chisholm; Gen Perkins" 
date: "7/18/2019"
output:
  pdf_document: default
  html_document: default
  word_document: default
---
```{r global_options, include=FALSE }
require(knitr)
#knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
#                      echo=FALSE, warning=FALSE, message=FALSE)
#knitr::opts_knit$set(progress=TRUE, root.dir = 'relative_path_to_root_from_Rmd')
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(clhs)
library(sf)
library(raster)
library(sp)
library(gdistance)
library(stars)
library(plyr)
library(dplyr)
library(foreach)
library(doSNOW)
library(stars)
library(iterators)
library(LearnGeom)
library(tmap)
library(viridis)
library(shinyjs)
library(formatR)
library (xlsx)
library(data.table)
library(tcltk)
library(fasterize)
library(stringr)
```

## Introduction
This document describes a Stage 1 field sampling strategy for application in machine-learning based predictive ecosystem map development.  The objectives of this sample design is to provide an efficient method for collection of unbiased field data for two applications: base-line training point sampling and internal accuracy assessment of the PEM map. 
A optimal sample strategy will balance requirements to be unbiased and statistically sound, to adequately sample the environmental space of the map area, to allow a scaleable number of samples to be selected in the design, and be as cost-efficient as possible.  To meet these competing requirements we apply a cost-constrained, sliced, conditional Latin Hypercube Sample (cLHS) approach based on landscape scale environmental variables to position paired equalateral triangle transects for line-intercept collection of information following a modified Moon et al methodology.
As the first stage of field data collection, the protocol is not based on existing site series mapping or presumed stand-level variables of importance. The conditioned Latin hypercube sample is based on landscape-scale and geographic variables to ensure coverage of broad physiographic and macroslope position in the map area. We propose to use a simplified Moon method to collect the transect data. The specific field methods are described in .....  In this approach, equilateral transects are traversed and changes from one ecosystem type to another marked along the transect. The GPS tracklog of the field-traverse is smoothed, buffered, and split into segments that designate a band of rasters reflective of individual map units.
These buffered line-intercept data will be used both a source of training points and for an internal accuracy assessment. A ratio of transects will be used for training point generation to build the map (70%) while the remainder will be used as AA polygons to test the accuracy of the modelled map. A boot-strapping procedure using different sets of transects to build and assess the model will provide a measure of variance for the accuracy.
1. For training point generation, rasters falling within the buffered line-intercept polygons representing each map unit will be used as a population from which a cLHS sample will be taken for use as training points representing the map unit in the randomforest model.   
2. For the internal AA, an overlay of a complete transect polygon over the PEM map will provide percent agreement statistics as per Moon. 

Stage 1 field sampling from this method is intented to provide core unbiased information for both of these tasks. This method will be most successful in sampling the common matrix ecosystems but may not satisfactorily sample or evaluate uncommon and small-patch ecosystems.  It is expected that additional training point data collection by other approaches be conducted concurrently or in the Stage 2:map improvement phase which may involve purposeful sampling of poorly sampled map units, additional transects in areas of high map uncertainty, etc.  These various "map-improvement" approaches to improving the training point data set will be laid out in Stage 2 sampling.
A Stage 1 air photo sampling protocol is outlined in.... This protocol is primarily designed for sampling small-patch ecosystems that are easily distinguishable for airphotos. 

## Main Steps to the Stage 1 Field Sampling Strategy

The script below generates a cLHS of points around which field transects are generated for use in field data collection PDF maps. 

There flow of the script follows these steps:

1. Create cost layer based on relative ease of access and to minimize sampling of recently cleared stands.
2. Select and generate a limited list of broadly defined landscape-scale spatial layers for use in the cLHS.
3. Create an exclusion layer of areas that should not be sampled, such as roads, lakes, etc.
4. Determine an optimal number of cLHS points required to fill the environmental space.
5. Create a raster stack of spatial and cost layers and mask with the exclusion layer.
6. Create a sliced cLHS of 5 sample points per slice.
7. Generate possible paired sample points adjacent to the cLHS samples at each cardinal and ordinal direction and remove those points falling outside the masked raster stack area.
8. Generate 250m equilateral triangle transects with the cLHS as centre points, rotate transects randomly, create 10m transect buffer layer.
9. Export 3 shape files into QGIS, underlay BING or other high-resolution imagery and generate 1:2 000 field sampling maps in QGIS using Atlas function for each transect.

Many of these algorithms are based on and adapted from those described in Malone et al (2019). 


## Step 1. Build Cost Layer

An accumulated cost surface is generated from the 25m DEM base. We used the Digital Road Atlas layer clipped to the study area to identify road and trail access.
Each 25m raster traversed was assigned the following costs:

  * any driveable roads = 0;
  * "quad-able"" tracks = 2.5
  * normal ground = 33
  * sloping ground increased by 1 for each % slope grade > 25%
  * points may also be placed that represent helicopter landing areas. These may be given a zero cost where helicopter access is already part of the field sample plan, or a cost that attempts to place points by road initially but will apply helicopter points only where the costs by road become excessive

The assignment of these cost values is heuristic but the setting above place points that equate well with the arbitary 250m road buffers applied in the first year sampling protocol.
Buffered clearcuts are now assigned a high cost to limit the number of transects which overlap cutover areas, but to allow the variable space occupied by clearcuts to be included in the calculations of the cLHS. Clearcuts are currently assigned a weight of 3000.

```{r setup user directories}
### change root dir only if the data is stored separately from the script
#### it is suggested that the subdirectory structure be retained
root_dir <- getwd()
#-------change if the input/out files data are stored elsewhere otherwise comment out ---------
#root_dir = "D:/GitHub/PEM_Research_BC/Deception_PEM/02Deception_SamplingPlan/021Stage1SamplePlan" ## 
#---------------
in_cost = paste(root_dir, "./inputs/CostSurface_layers/Deception", sep = "") ## folder with files for cost surface calc
in_var = paste(root_dir, "/inputs/cLHS_variables", sep = "") ## folder with variables used in cLHS
out_dir = paste(root_dir, "/outputs/Stage1_StudyDesign", sep = "") ## folder for ouput of script
```

```{r source transect functions}
## load functions stored in...
source ("./_Transect_Functions.R")
```


```{r select run options}
BGC_choose <- "SBSmc2" ###select BGC for current cLHS run
```

```{r cost, tidy = TRUE, warning=FALSE}
### add in function to skip code if file already
#####read in road layer clipped from Digital_Road_Atlas
roads <- st_read(dsn = in_cost,layer = "Deception_Atlas_Roads_Cleaned", quiet = TRUE) %>% st_set_crs(3005)# read in classified roads
roads <- roads["ROAD_SURFA"]
#roads <- st_read(dsn = "CostSurface_layers/DeceptionClips_June28.gdb",layer = "Digital_Road_Atlas_Deception", quiet = TRUE) # read in classified roads
#roads <- roads["ROAD_CLASS"]
# roadsExtra <- st_read(dsn = "DeceptionRoads",layer = "Deception_Access_RoadAtlas") ## bigger road layer
# roadsExtra <- roadsExtra["DESCRIPTIO"]
boundary <- st_read(dsn = in_cost,layer = "DeceptionProjectBoundary", quiet = TRUE) # read in classified roads
area <- raster(paste (in_cost, "./slope_25m.tif", sep = "")) ##read in slope raster
roads <- st_buffer(roads, dist = 25) %>% st_cast("MULTIPOLYGON")
##assign cost to each road type
roads$ROAD_SURFA <- ifelse(roads$ROAD_SURFA == "seasonal" | roads$ROAD_SURFA == "rough" | roads$ROAD_SURFA == "unknown"  ,2.5,
                           ifelse(roads$ROAD_SURFA == "loose" | roads$ROAD_SURFA == "paved",0,0))
rds1 <- fasterize(roads,area,field = "ROAD_SURFA")###convert to raster
##read in road start points ? this needs to be improved for automatically identifying where roads begin
start <- st_read(dsn = in_cost, layer = "DecStartPoints", quiet = TRUE) %>% st_set_crs(3005) 
### currently can add start points for helicopter landing spots these will be zero cost
###but better would be to allow assignment of a separate starting value point for these to help balance the costs of helicopter versus walking long-distances from roads.
start <- as(start, "Spatial")
aCost <- tan(area)*100 # radians to grade
aCost[aCost < 25] <- 33 # base cost per 25m pixel increases by 1 for every % above 25%
em <- merge(rds1,aCost) # combine
tr <- transition(em, transitionFunction = function(x) 1/mean(x), directions = 8) # create  transition layer
aCost <- accCost(tr,start) # create accumulated cost lyaer
#aCostall <- aCost
cutblks <- st_read(dsn = in_cost, layer = "Deception_All_YoungCuts", quiet = TRUE) # read in cutblock layer
cutblks$ROAD_SURFA <- ""
cutblks <- cutblks["ROAD_SURFA"]
cutblks <- st_buffer(cutblks, dist = 150) # add buffer to limit possibility of sampling running into cutblocks
cutblks$ROAD_SURFA <- 3000
cutblks <- st_rasterize(cutblks)
cutblks  <- as(cutblks ,"Raster")
cost1 <- projectRaster(cutblks, aCost)
aCostall <- merge (cost1, aCost)
writeRaster(aCostall, filename= paste (in_cost, "./AccumCostSurface.tif", sep = ""), format="GTiff", overwrite=TRUE)
##########################################################
###jump to here if cost surface already built prior lines#
##########################################################
#aCost <- raster("CostSurface_layers/AccumCostSurface.tif") # read previously created cost surface
```

```{r costmap, tidy = TRUE, echo=F}
#tmaptools::palette_explorer() ###tool for choosing colours
tbound <- tm_shape (boundary) + tm_borders()
troad <- tm_shape (roads) + tm_polygons()
tmcost <- tm_shape (aCostall) + tm_raster("layer",  title = "Accumulated Cost", palette = "YlGnBu", n=20, legend.show = F)  + tm_layout(main.title = "Cost Layer and Roads for Deception Lake", main.title.size = .75)
tmcost2 <- tmcost + troad + tbound
tmcost2
```

## Step 2. Select and Stack Spatial Data Layers for cLHS
Spatial layers from the 25m raster set were used in the cLHS. Three raw continuous variables were included in the cLHS : Elevation, Latitude, Longitude. 
In addition, three derived 25m variables were used as categorical variables::
  + MRVBF classes were used with an initial starting slope of 64, with lower slope threshold = 6 and upper slope = 2. These setting provided good differentiation of the entire landbase compared to the default settings which emphasize areas of sediment deposition in lower slope positions only.
  + The default Land Form Class derived from TPI and TRI metrics was applied but was sieved to 4 rasters to remove small raster clusters.
  + Diurnal anisitropic heating was converted into three classes reflecting cold, warm, and neutral slopes. The cut-offs were for cool were < -0.2 and for warm >0.2

These variables are largely uncorrelated with each other. Topographic Wetness was suggested as a variable for use but correlates highly with MRVBF and also does not produce large landscape level units but operates at a stand level: not included.

```{r dataPrep, tidy = TRUE}
#list of ancillary data to be used in cLHC all at 25m raster size
rastList <- list.files(in_var) %>% paste(in_var, "/", .,  sep = "") 
              ## 25m_MRVBF_Classified_IS64Low6Up2.tif",
              ## 25m_DAH_3Class.tif",
              ## 25m_LandformClass_Default_Seive4.tif",
              ## DEM.tif")
numAnc <- length(rastList) ## read rasters of ancillary data
ancDat <- stack()
for(i in 1:numAnc){
  temp <- raster(rastList[i])
  ancDat <- stack(ancDat,temp)
}
```

```{r table, echo=F}
jnk=layerStats(ancDat, 'pearson', na.rm=T)
corr_matrix=jnk$'pearson correlation coefficient'
corr_matrix <- as.data.frame(corr_matrix)
corr2_matrix <- corr_matrix %>% dplyr::rename(
MRVBF = MRVBF_Classified_IS64Low6Up2_25m,
DAH = DAH_3Class_25m,
LandForm = LandformClass_Default_Seive4_25m)
knitr::kable(corr2_matrix, caption = "Correlation between cLHS variables")
```

```{r variablemaps, tidy = TRUE, echo=FALSE, message=FALSE}
#tmaptools::palette_explorer() ###tool for choosing colours
tbound <- tm_shape (boundary) + tm_borders()
tm1 <- tm_shape (ancDat) + tm_raster("LandformClass_Default_Seive4_25m", palette = "inferno", title = "LandformClass") + tbound
tm2 <- tm_shape (ancDat) + tm_raster("MRVBF_Classified_IS64Low6Up2_25m", palette = "viridis", title = "MRVBFClass") + tbound
tm3 <- tm_shape (ancDat) + tm_raster("DAH_3Class_25m", palette = "RdBu", title = "DAHrClass") + tbound
tm4 <- tm_shape (ancDat) + tm_raster("DEM_25m", palette = "YlOrBr", title = "Elevation") + tbound
tmap_arrange(tm1, tm2,tm3,tm4)
```

## Step 3. Create Exclusion Layer of No-Sample Areas

We are placing cLHC locations as centre points for 250m per-side transect triangles. The radius of a circle that will encompass and rotation of the triangle is 144m. We mask the spatial layers with an exclusion layer created from buffering features we do not wish to traverse by this radius. cLHS this masked spatial layer ensures that the placement of centre points from the cLHS will not result in traverses overrunning unsampleable lakes, roads, etc.  
We apply a buffer around roads to 175m to account for any wider right-of-ways. 

```{r costBuffer, tidy = TRUE}
#BGC_choose <- "SBSmc2" ###select BGC for cLHS run
lakes <- st_read(dsn = in_cost, layer = "VRI-Lakes", quiet = TRUE) # read lakes shapes to exclude from sampling
lakes <- lakes["POLYGON_ID"]
decBGC <- st_read(dsn = in_cost, layer = "BGC_Deception_Dissolved_fixed", quiet = TRUE) # clipped shape file of area BGCs
BGC <- decBGC[decBGC$MAP_LABEL %in% BGC_choose,"MAP_LABEL"] # Select BGC
##Mask out buffer around areas not desired to be sampled.
lakesBuff <- st_buffer(lakes, dist = 145) # lakes and clearcuts
roadsBuff <- st_buffer(roads, dist = 160) # roads
BGCBuff <- st_buffer(BGC, dist = -150) # prevents transects from crossing BGC lines
temp <- fasterize(BGCBuff, aCostall) # mask doesn't work properly with polygon so convert to raster
##mask out areas
aCostall2 <- mask(aCostall,temp)
aCostall2 <- mask(aCostall2,roadsBuff,inverse = TRUE)
aCostall2 <- mask(aCostall2,lakesBuff, inverse = TRUE)
##save clipped cost surface
#writeRaster(aCost, filename="CostSurface_layers/AccumCostBuffered.tif", format="GTiff", overwrite=TRUE)
##create polygon of buffered area to increase speed
polyBuff <- aCostall2
polyBuff[is.infinite(values(polyBuff))] <- NA
polyBuff[!is.na(values(polyBuff))] <- 1
polyBuff <-st_as_stars(polyBuff) %>%
  st_as_sf(as_points = F, merge = T, na.rm = T, use_integer = T)
plot (aCostall2)
```
## Step 4.Create Raster Stack and Mask with Exclusion Layer

```{r clip_to_samplearea, tidy = TRUE}
### Prepare data for cLHS and sample number optimisation
ancDat <- stack(ancDat, aCostall2) # add cost surface to stack
ancDat <- crop(ancDat,aCostall2)
ancDat <- mask(ancDat,aCostall2) # mask by buffered cost surface
ancDat$DEM <- setValues(ancDat$DEM_25m,signif(values(ancDat$DEM_25m),2)) # downscale DEM resolution
#rename (s, c("AccumCostSurface"="layer") )
s <- sampleRegular(ancDat, size = 500000, sp = TRUE) # sample raster
s <- s[!is.na(s$layer) & !is.na(s$DEM_25m),] # remove cells outside of area
for(i in c(1,3)){ # convert some variables to factor
  s@data[,i] <- as.factor(s@data[,i])
}
rownames(s@data) <- NULL
s@data <- cbind(s@data,s@coords) # add lat and long
```


## Step 5.  Calculate Optimal Number of cLHS Points

None of the optimization methods suggested in literature worked properly with categorical variables. Kiri built a new approach which calculates the proportion decrease in the objective function with increasing numbers of samples. 
For the SBSmc2 of the Deception map area 25 samples was deemed to be sufficient to sample 90% of the 6 variable environmental space.
```{r sampleNum, tidy = TRUE, echo=FALSE}
###may want to run function in next line to choose number of sample points
  num <- numSamples(ancDat) ### this function may take a little time applied to entire raster
  cat("90 percentile is:",num)
  #mSamp <- numSamples(ancDat = s) ### same function but applied to a subset of raster
  #cat("90 percentile is:",mSamp)
  plot
```


## Step 6. Run Cost-Constrained Sliced cLHS

A cLHS can now be run on this constrained sample space and any point selected will be support a valid transect no matter the rotation. 
cLHS sets are generated for each BGC individually.
We apply a minimum 1000m spacing between cLHS points to prevent possible overlap of paired transects.

We created six cLHS 'slices' of 5 sites (n=30) for sampling. Each slice represents a cLHS independently so all sites from a slice must be sampled. Strictly speaking the slices must be sampled in order to maintain LHS structure as well (i.e. 1 + 2 + 3 = LHS but 1 + 2 + 4 may not be LHS). A progressive cLHS procedure has been proposed that makes any combination of slices maintain LHC structure but this is not yet available in R. Additional new cLHS slices can be created after the generation of the original run using the 'include' parameter in the cLHS function. Where sampling may involve helicopter access, additional slices could be added using a cost layer where landable clearings rather than roads are the zero cost starting points.


```{r cLHS_Sliced, warning=FALSE, error=FALSE}
######Set parameters for sliced cLHS 
##total sample size
maxSample <- 25
##slice size
sliceSize <- 5
##radius of exclusion around points to prevent overlap of secondary transects
radExc <- 1100
numSlice <- (maxSample)/sliceSize
t1 <- clhs(s, size = 5, iter = 100, simple = F, progress = F, cost = 'layer') ### create first slice. layer = cLHSAccumCostSurface. Use 5000 iterations for a normal run
 
sPoints <- t1$sampled_data
sPoints$SliceNum <- 1
# loop to create all slices
for(i in 2:numSlice){ ###loops and create new slices
  for(pt in rownames(sPoints@data[sPoints$SliceNum == (i-1),])){ ##remove data in zone of exclusion
    tDist <- try(spDistsN1(pts = s, pt = s[pt,]))
    if(class(tDist) != "try-error")s <- s[tDist > radExc | tDist == 0,]
    else{
      i <- i - 1
      sPoints <- sPoints[sPoints$SliceNum != i,]
    }
  }
  temp <- clhs(s, size = sliceSize*i, iter = 100, simple = F, progress = F,cost = 'layer', ## cLHS for new slice including all previous slices. Use 5000 iternations for a normal run
               include = which(rownames(s@data) %in% rownames(sPoints@data)))
  temp <- temp$sampled_data
  temp <- temp[!rownames(temp@data) %in% rownames(sPoints@data),] # remove data from previous slices
  temp$SliceNum <- i
  sPoints <- rbind(sPoints, temp) ## add slice to set
}
####for adding an additional slice to an existing cLHS
  #oldslice <-  which(rownames(s@data) %in% rownames(sPoints@data))
# size should be the size or the old slide plus the number for the new slice. In the example below 25 + 5  
#newslice <-  clhs(s, size = 30, iter = 1000, simple = F,
      #progress = F, cost = 'layer', include = oldslice)
samplePoints <- st_as_sf(sPoints) 
### add index values
rownames(samplePoints) <- NULL
samplePoints$Subzone <- BGC_choose
samplePoints$SliceSite <- rep(1:sliceSize,numSlice)
samplePoints$TotNum <- 1:maxSample
samplePoints$ID <- paste(samplePoints$Subzone,"_",samplePoints$SliceNum, ".",samplePoints$SliceSite,"_",samplePoints$TotNum, sep = "")
samplePoints <- samplePoints[,-c(1:7)]
```


## Step 7. Generate Location Options for Paired Samples

For sampling efficiency and field safety paired transects at a cLHS points is desireable. The cLHS point is the centre point of the first transect. We generate additional points at 400m from the cLHS points in the eight cardinal and ordinal directions . This distance provides a minimum 100m spacing from the cLHS transect to any second transect. Cardinal points that fall outside the buffered cost surface are removed as candidates.
Selection of the second (or third) transect from the available set could be made in several ways:randomly, choice of least cost, transect with maximum variable difference from cLHS point.

``` {r pairedLoc, tidy = TRUE}
###First, create 8 cardinal-ordinal points and mask
samplePointsShort <- samplePoints[,c("ID","geometry")]
rotStandard <- c(0, 45, 90, 135, 180, 225, 270, 315) # rotation degrees 
pntsOut <- foreach(pnt = iter(samplePointsShort, by = 'row'), .combine = rbind) %do% {
  rotPts <- pairedPoint(POC = pnt, Dist = 400, Rotations = rotStandard) ##8 points 400 m away
  rotPts <- st_transform(rotPts, st_crs(polyBuff))
  rotPts <- st_join(rotPts, polyBuff, join = st_intersects) ##remove points not in buffered area
  rotPts <- rotPts[!is.na(rotPts$layer),-3]
  rotPts
}
pntsOut$Rotation <- mapvalues(pntsOut$Rotation,c(0, 45, 90, 135, 180, 225, 270, 315),c("N","NE","E","SE","S","SW","W","NW")) ## change names
########select paired trasect at random
      #pntsOutRnd <- pntsOut %>% group_by(ID) %>% sample_n(size = 1) ## select a paired point at random
        #pntsOutRnd <- ungroup(pntsOutRnd)
### Select paired point by least cost
pntsOutxy <- st_coordinates(pntsOut)
pntsOutMincost <- cbind(pntsOut, (raster::extract(aCost, pntsOutxy)))
names (pntsOutMincost) [3] <- "Cost"
pntsOutMincost2 <-ddply(pntsOutMincost, "ID", subset, Cost == min(Cost))
#pntsOut_mincost <- pntsOut[interaction (pntsOut[,c("ID", "Rotation")]) %in% interaction (pntsOutMincost2[,c("ID", "Rotation")]) ,]
pntsOut_mincost <-pntsOut[match(paste(pntsOutMincost2$ID, pntsOutMincost2$Rotation), paste(pntsOut$ID, pntsOut$Rotation),),]
## select a paired point by minimum cost
samplePointsShort$Rotation <- "cLHS"
allPnts <- rbind(samplePointsShort,pntsOut)
allPnts$ID <- paste(allPnts$ID,allPnts$Rotation, sep = "_")
### This is all possible paired samples
#samplePnts <- rbind(samplePointsShort,pntsOutRnd) ## This includes a randomly sampled single paired sample
#samplePnts <- allPnts ##for reporting all possible sample pairs
samplePnts <- rbind(samplePointsShort,pntsOut_mincost) ## This includes a minumum cost single paired sample
samplePnts$ID <- paste(samplePnts$ID,samplePnts$Rotation, sep = "_")
```



## Step 8. Generate Transects Around cLHS and Paired Points

The cLHS and paired points are used as the __centre point__ for the 250m-per-side transect triangles. For Stage 1 sampling, we rotate each of the transects randomly. Alternatively, optimized rotation to maximize diversity of the traverse can be applied for other purposes such as map improvement sampling (Stage 2).

``` {r triTrans, tidy = TRUE}
###Second, create triangle around each point and randomly rotate
#allTri <- foreach(pnt = iter(allPnts, by = 'row'),.combine = rbind) %do% { ##use this for all pair options
allTri <- foreach(pnt = iter(allPnts,  by = 'row'),.combine = rbind) %do% { 
  POC <- pnt
  pnt <- cbind(pnt, st_coordinates(pnt)) %>% st_drop_geometry()
  temp <- Tri_build(ID = 1, x = pnt$X, y = pnt$Y) %>%
    st_as_sf()
  rotDeg <- runif(1, min = 0, max = 360)
  temp <- rotFeature(temp, POC, rotDeg)
  if(is.na(POC$Rotation)){
    temp$ID <- pnt$ID
 }else{
   temp$ID <-  pnt$ID #paste(pnt$ID,pnt$Rotation, sep = "_")
  }
  temp
}
sampleTri <- allTri[allTri$ID %in% samplePnts$ID,]
```

``` {r cLHS_map, tidy = TRUE, echo=F, out.width = "90%"}
#tmaptools::palette_explorer() ###tool for choosing colours
tbound <- tm_shape (boundary) + tm_borders()
tcostclip <- tm_shape(aCostall) + tm_raster(palette = "YlGnBu",n=20, contrast = c(0,1),  legend.show = F)
tclhs <- tm_shape(samplePoints) + tm_dots(palette = "Set1", group = "SliceNum", scale=3)
tclhsmap <-  tcostclip + tbound + tclhs
tclhs2 <- tm_shape(allPnts) + tm_dots(palette = "Set1", group = "SliceNum", scale=1)
tclhsmap2 <-  tcostclip + tbound + tclhs2
tclhs3 <- tm_shape(allTri) + tm_lines(palette = "Set1", group = "SliceNum", scale=1)
tclhsmap3 <-  tcostclip + tbound + tclhs3
tmap_arrange (tclhsmap , tclhsmap2)
```

```{r cLHS_map2, echo = F, out.width = "125%"}
tbound <- tm_shape (boundary) + tm_borders()
tclhs3 <- tm_shape(sampleTri) + tm_lines(palette = "Set1", group = "SliceNum", scale=1) + tm_layout(main.title = (paste ("cLHS Sample Triangles for ", BGC_choose, "of Deception Lake")), main.title.size = .75)
tclhsmap3 <-  tcostclip + tbound + tclhs3
tclhsmap3
```

## Step 9. Export Transects to QGIS for Field Maps

Transects are uniquely labelled by Subzone-Slice.slice site number-Running number-Cardinal direction (e.g. SBSmc2_6-3_28_NW). Three shape files are exported for use in QGIS: Centre points, transect triangle, and 20m buffer polygon (field sampling boundary). Field maps for Avenza PDF maps are generated in QGIS or ARC. In QGIS, the Atlas function will generate a PDF for each unique point in the cLHS point file. We used a BING base layer (alternate is GOOGLE or orthphoto mosaic) and build QGIS styles to theme the transect and buffer. A map scale of 1:2 000 seems about right for field application. An overview PDF map of transect locations is required for route finding to the transects.
```{r fieldmap, echo=T}
#Example QGIS output.
knitr::include_graphics(paste(root_dir, "/SBSmc2_1.4_4_cLHS.pdf", sep = ""))
#![optional caption text](SBSmc2_1.4_4_NA.pdf)
```


``` {r export, tidy = TRUE, echo=F, message = FALSE, include = FALSE}
#####write Transects####################
 st_write(allTri, dsn = out_dir,
          layer = paste(BGC_choose,"_allTransectPairs_", Sys.Date(),sep = ""), driver = "ESRI Shapefile",
          update = T, delete_layer = T)
 st_write(sampleTri, dsn = out_dir,
          layer = paste(BGC_choose,"_sampleTransectPairs_", Sys.Date(),sep = ""), driver = "ESRI Shapefile",
          update = T, delete_layer = T)
####write buffer#########################
triBuff <- st_buffer(allTri, dist = 10)
 st_write(triBuff, dsn = out_dir,
          layer = paste(BGC_choose,"_allTransectBufferPairs_", Sys.Date(), sep = ""),
          driver = "ESRI Shapefile", update = T, delete_layer = T)
 sampleBuff <- st_buffer(sampleTri, dist = 10)
 st_write(sampleBuff, dsn = out_dir,
          layer = paste(BGC_choose,"_sampleTransectBufferPairs_", Sys.Date(), sep = ""),
          driver = "ESRI Shapefile", update = T, delete_layer = T)
####write points#######################
fields <- st_drop_geometry(allPnts)
allPnts <- merge(allPnts, fields, by = "ID", all.x = T)
#samplePnts$ID <- paste(samplePnts$ID,samplePnts$Rotation, sep = "_")
st_write(allPnts, dsn = out_dir,
         layer = paste(BGC_choose,"_allPointPairs_",  Sys.Date(), sep = ""),
         driver = "ESRI Shapefile", update = T, delete_layer = T)
 
fields2 <- st_drop_geometry(samplePnts)
samplePnts <- merge(samplePnts, fields2, by = "ID", all.x = T)
#samplePnts$ID <- paste(samplePnts$ID,samplePnts$Rotation, sep = "_")
st_write(samplePnts, dsn = out_dir,
         layer = paste(BGC_choose,"_samplePointPairs_",  Sys.Date(), sep = ""),
         driver = "ESRI Shapefile", update = T, delete_layer = T)
####write study design spreadsheet
samplePnts2 <- st_transform(samplePnts, 4269)
Sampledesign <- cbind(st_drop_geometry(samplePnts2), st_coordinates(samplePnts2))
Sampledesign[, c("Surveyor", "DateCompleted", "TransectComments")] <- ""
#Sampledesign$BGC <- str_split_fixed(Sampledesign$ID, "_", 3)
write.csv (Sampledesign, file = paste (out_dir,"Deception_", BGC_choose, "_SampleDesign.csv"),
                                       row.names = FALSE)
```


## Subsequent Scripts 
1. Import and Process Transect data. (Done. See Stage1TransectImport.R)

  + Download GPS track logs and way points by transect from zipped shape file export from Avenza maps Schema
  + Link together field transect placemarks and split by placemark
  + Create raster from buffered transects to base layer with rasters identified by Map unit call
  + Compile a raster layer of all field identified rasters to map unit

2. Script to both Extract Training points from Transects for use in machine-learning and as AA transects to test map

  + Build initial machine-learning model from a random sample of training points to identify the primary variables of importance
  + cLHS sample the raster population representing each map unit using the variables identified in step 1. Some balancing of training points may be required.

3. Overlay transect polygons over modelled map layer for internal accuracy assessment

4. Select training points and build map from all sites leave-one-out and test the map accuracy with withheld site pair of polygons

  + Bootstrap this procedure for all site pairs


***
## Functions applied from Transect_Functions.R script but shown here
```{r functions for view, tidy = TRUE, eval = F}
## Math needed to rotate
rot <- function(a) matrix(c(cos(a), sin(a), -sin(a), cos(a)), 2, 2)
## Feature rotation
rotFeature <- function(Feature, PivotPt, Bearing) {
  # where Feature is the Shape to be rotated, eg:  #Feature <- tri
  # Bearing is the compass bearing to rotate to    #PivotPt <- pt.sf
  # PivotPt is the point to rotate around          #Bearing <- 15 
  
  ## extract the geometry
  Feature_geo <- st_geometry(Feature)
  PivotPoint  <- st_geometry(PivotPt)
  
  ## Convert bearing from degrees to radians
  d <- ifelse(Bearing > 180, pi * ((Bearing-360)/ 180) ,  pi * (Bearing / 180))
  
  rFeature <- (Feature_geo - PivotPoint) * rot(d)   + PivotPoint
  rFeature <- st_set_crs(rFeature, st_crs(Feature))
  
  Feature$geometry <- st_geometry(rFeature) ## replace the original geometry
  return(Feature)
}
pairedPoint <- function(POC, Dist, Rotations){ #Where bearing is the bearing recorded in the transect
  # This function is dependent on other PEM functions: rot, rotFeature
  PROJ <- st_crs(POC)
  pGeom <- st_geometry(POC)
  pGeom <- pGeom + c(0,Dist)
  feat <- st_set_geometry(POC, pGeom)
  pts <- foreach(Bear = Rotations, .combine = rbind) %do%{
    temp <- rotFeature(feat, POC, Bear)
    temp <- st_set_crs(temp, PROJ)
    temp$Rotation <- Bear
    temp
  }
  return(pts)
}
Tri_build <- function(ID, x, y){
  tris <- CreateRegularPolygon(3, c(as.numeric(paste(x)), 
                                    as.numeric(paste(y))), 145) # number of sides, center pt and length of sides
  
  MoonLineCentre <- data.frame(tris) %>%
    st_as_sf(., coords = c("X", "Y"), crs = 3005) %>%
    mutate(ID = ID) %>%
    group_by(ID) %>%
    summarize() %>%
    st_cast("POLYGON") %>%
    st_cast("MULTILINESTRING") #%>%
  #st_set_crs(newproj)
  return(MoonLineCentre)
} 
numSamples <- function(ancDat) {
  tempD <- as.data.frame(ancDat)
  tempD <- tempD[!is.na(tempD[,2]),-length(tempD)]
  for(i in c(1,3)){ ##convert to factor
    tempD[,i] <- as.factor(tempD[,i])
  }
  nVars <- ncol(tempD)
  df <- tempD
  cl <- makeCluster(7)
  registerDoSNOW(cl)
  iterations <- 12
  pb <- txtProgressBar(max = iterations, style = 3)
  progress <- function(n) setTxtProgressBar(pb, n)
  opts <- list(progress = progress)
  ###loop through and calculate change in objective function
  cseq <- seq(10,200,10)
   sampSize <- foreach(ss = cseq,.combine = rbind, .packages = c("clhs")) %dopar% {
    res2 <- clhs(s, size = ss, iter = 3000, progress = F, simple = FALSE, cost = 'layer')
    data.frame(nSample = ss, deltaObj = min(res2$obj)/max(res2$obj))
  }
  
  stopCluster(cl)
  
  ###Now fit data to exponential function
  x <- sampSize$nSample
  y <- sampSize$deltaObj
  
  #Parametise Exponential decay function
  plot(x, y, xlab="sample number",ylab = "Prop Obj-fun Decrease")          # Initial plot of the data
  start <- list(k = 100,b1 = 0.05,b0 = 100)
  fit1 <- nls(y ~ -k*exp(-b1*x) + b0, start = start)
  lines(x, fitted(fit1), col="red")
  
  xx<- seq(1, 200,1)
  jj <- predict(fit1,list(x=xx))
  normalized = (jj-min(jj))/(max(jj)-min(jj))###standardise
  return(approx(x = normalized, y = xx, xout = 0.95)$y)###get 95th quantile
}
```
