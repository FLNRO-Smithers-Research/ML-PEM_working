---
title: "Build Machine-learning Predictive Ecosystem Map - Forest-Nonforest"
subtitle: "by Will MacKenzie"
Methods author: "Will MacKenzie"
Script author: "Will MacKenzie; Gen Perkins; Kiri Daust"
date: "9/12/2021"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r global_options, include=FALSE }
require(knitr)

```

```{r setup, include=FALSE}

library(data.table)
library(scales)
library(caret)
library(sf)
library(ranger)
library(tidyverse)
library(fasterize)
library(stringr)
library(dplyr)
library(raster)
library(readxl)
library(foreach)
library(tidymodels)
library(themis)
library(vip)
library(stringi)
library(R.utils)
library(colorspace)

#devtools::install_github("tidymodels/tune")
#devtools::install_github("tidymodels/parsnip")

```

# Introduction

This script uses training point samples collected from Stage 1 transects and other point data collected.

This is by boot strapping the transects where 90% are use to provide training points to the build a map, and 10% are withheld and used as AA transects. 
This analytical approach is then used to guide improvements to the map.

1. Load data prepared in [04a script](https://github.com/bcgov-c/BEC_DevExchange_Work/blob/master/04a_TrainingPt_TransectImport%26Clean.Rmd). 
2. Split dataset into training and testing sets
3. Run the model and record model performance metrics
4. Best model is selected to create a map and associated entropy layer 


Prepare directories

```{r session setup, tidy = TRUE, warning=FALSE}

AOI <- "Deception"
#AOI <- "BoundaryTSA"

# set up file structure
AOI_dir <- file.path(".", paste0(AOI,"_AOI"))
cov_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")
shapes_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers")
input_pnts_dir <- file.path(AOI_dir, "1_map_inputs", "trainingData")
out_dir <- file.path(AOI_dir, "3_maps_analysis","models")

# read in temp functions
source(here::here('_functions', 'model_gen_tidy.R'))
source(here::here('_functions', 'acc_metrix.R'))
source(here::here('_functions', 'doc_theme_pem.R'))
source(here::here('_functions', 'balance_recipe.R'))

# read in map and model keys
map.key  <- read.csv(file.path(AOI_dir, "_MapUnitLegend", 
                                 paste0(AOI, "_MapUnitLegend.csv")), 
                       stringsAsFactor = FALSE)

  fMat <- read.csv(file.path(AOI_dir, "_MapUnitLegend", 
                                  "fuzzy_matrix_basic.csv")) %>%
  dplyr::select(c(target, Pred, fVal))
  
  
 fMat <- data.table(fMat)
# read in model parameters 
model_param <- file.path(AOI_dir, "_MapUnitLegend", "models.xlsx")

# set up model parameters:  
mparam <- read_xlsx(model_param, "models") %>% filter(to_run == 1)
map_res <- mparam$resolution
data_res <- paste0("att_", map_res, "m")
mname <- paste0(mparam$model_name)
mrep <- mparam$model_rep

# check which catergory of model to be produced
mtype <- case_when(
  str_detect(mname, "for_nf")  ~ "forest_non_forest",
  str_detect(mname, "nf_") ~ "non_forest",
  str_detect(mname, "fore") ~ "forest"
)

# get covariates
mcov <- read_xlsx(model_param, "covariates", skip = 2) %>%
  filter(!!sym(mparam$covariates) == 1) %>%
  dplyr::select(covariate)

# get training point sets
mtpt <- read_xlsx(model_param, "training_pts", skip = 2) %>%
  filter(!!sym(mparam$training_pts) == 1) %>%
  dplyr::select(tp_code)%>%
  pull

# get the map unit level 
mmu <- read_xlsx(model_param, "map_unit", skip = 2) %>%
   filter(!!sym(mparam$map_unit) == 1) %>%
  dplyr::select(legend_column)%>%
  pull

mmu <- case_when(
  mmu == "column_mu" ~ "MapUnit", 
  mmu == "column_ss" ~ "SiteSeries",
  mmu == "column_ass" ~ "Association",
  mmu == "column_cls" ~ "Class",
  mmu == "column_grp" ~ "Group",
  mmu == "column_typ" ~ "Type"
)

# set up outfolder: 

if(!dir.exists(file.path(out_dir, mtype))){dir.create(file.path(out_dir, mtype))} 

out_dir <- file.path(out_dir, mtype, mname, mrep) 

# set up model folder: 
if(!dir.exists(out_dir)){
  dir.create(out_dir)} else { 
    print ("folder already exists for this model name - are you sure you want to overwrite the results?")}

```


##Create raster stack of spatial layers

```{r create raster stack of spatial variables}

res_folder <- paste0(map_res, "m") # or for trim testing use line below
#res_folder <- paste0(map_res, "m_trim")

rast_list <- list.files(file.path(cov_dir, res_folder), pattern = ".tif$", full.names = TRUE)

# filter based on covariate for model param
rast_list <- rast_list[tolower(gsub(".tif", "", basename(rast_list))) %in% tolower(mcov$covariate)]

mcols <- gsub(".tif","", tolower(basename(rast_list)))

bec_shp <- st_read(file.path(shapes_dir, "bec.gpkg"), quiet = TRUE)


```


##Prepare training and testing datasets for the model

Using the point data generated in various forms from the 04a script, iteratively read in those points and create and save models. Model data is saved in a spreadsheet so that the spreadsheet can be filtered later to find the model with the best accuracy metrics. If the model is broken down by subzone and you have lots of different input datasets, this could take a while!

The random forest model is created using the training data set from above. Repeated cross validation is used to generate model metrics, though remember this is using the training set. We will evaluate our own metrics using the held back test set that was created above.

The model is able to be fine tuned quite nicely. Certain preprocessing parameters are included here to limit the amount of data that is used in the final model, including a correlation cutoff (remove covariates that are highly correlated), nero-zero variance detection (removes covariates that have 0 or close to 0 variance), centering and scaling of the data, and more, though these are the more common setup parameters. Having this prevents actual preprocessing to occur and allows for much more efficient processing.

Using the test dataset, the model is evaluated using the held back points. A confusion matrix is generated and then saved to the model output folder.


```{r read in datasets }
# read in sets of data 

  indata <- list.files(file.path(input_pnts_dir, data_res), paste0(mtpt,"_att.*.gpkg$"), full.names = TRUE)
  
  tpts <- st_read(indata[1]) 
  
  infiles <- basename(indata) 
  
  
#  MU_count <- tpts %>% dplyr::count(mapunit1)
table(tpts$mapunit1)

# match to the key and filter for forest and non_forest points
subzones <- unique(bec_shp$BGC_LABEL)
subzones <- gsub("\\s+","",subzones )

tpts  <- tpts %>%
  cbind(st_coordinates(.)) %>%
  st_join(st_transform(bec_shp[, "BGC_LABEL"], st_crs(.)), join = st_nearest_feature) %>%
  st_drop_geometry() %>% 
  dplyr::rename(bgc_cat = BGC_LABEL) %>% 
  rename_all(.funs = tolower) %>% 
  droplevels()

# match the column for map unit based on key 
# select the target column using the mapkey if needed: 
map.key.sub <- map.key %>%
      dplyr::select(BaseMapUnit, !!sym(mmu)) %>%
      distinct()
  
tpts <- tpts %>% left_join(map.key.sub, by = c("mapunit1" = "BaseMapUnit")) %>%
    left_join(map.key.sub, by = c("mapunit2" = "BaseMapUnit")) %>%
    dplyr::select(-mapunit1, -mapunit2) %>%
    dplyr::rename("mapunit1" = MapUnit.x,
                  "mapunit2" = MapUnit.y) %>%
    dplyr::select(mapunit1, mapunit2, everything())

  
# Join the forest type call: fix the calls with primary and secondary are forest/ non forest conflict:

# Might need to still add something to split out where primary and secondary calls are different (forest and non-forest? )

# Not sure how to deal with this yet? - Just keep the primary call for now  
  
map.key.type <- map.key %>%
  dplyr::select(BaseMapUnit, ForNonfor) %>%
  distinct() %>%
  dplyr::filter(!is.na(ForNonfor))

tpts_nf <- tpts %>%
  left_join(map.key.type, by = c("mapunit1" = "BaseMapUnit")) %>%
  mutate(mapunit1_type = ForNonfor) %>%
  dplyr::select(-ForNonfor) %>%
  dplyr::select(mapunit1_type, mapunit1, mapunit2, everything())

# convert all the non_forest to a generic non_forest  
# if the mapuni1_type = NFor, Wat, 
    
tpts_nf1 <-  tpts_nf %>%
  mutate(mapunit1 = case_when(
    mapunit1_type == "Nfor" ~ "nfor",
    mapunit1_type == "Non_veg" ~ "non_veg",
    TRUE ~ mapunit1)
  )
    
tpts <- tpts_nf1 %>% dplyr::mutate(target = as.factor(tpts_nf1$mapunit1_type))

# filter columns
mpts <- tpts %>%
     dplyr::select(target, tid, slice, bgc_cat, any_of(mcols)) %>% 
    droplevels() 
  
#write_csv(mpts, "s1_clean_pts_all_fnf.csv")

```

Run the forest models - either for all units or per BGC. 

```{r run model}
# # if the name contains BGC then run models per BGC else run all
 if (str_detect(mname, 'bgc')) {
    
  zones <- c(as.character(subzones))
  
  bgc_pts_subzone <- lapply(zones, function (i){
      #i <- zones[11]
      pts_subzone <- mpts %>%
        filter(str_detect(target, as.character(paste0(i, "_")))) %>%
        #drop_na() %>%
        droplevels()
      
      if(nrow(pts_subzone) == 0){ pts_subzone = NULL} else {ppts_subzone = pts_subzone }
      pts_subzone
  })
  
  # generate a name for list objects removing NULL values
  names <- unlist(lapply(bgc_pts_subzone, is.null))
  zone_names <- zones[-(which(ll <- names)) ] 
  
  # remove null or missing subzones data sets 
  bgc_pts_subzone =  bgc_pts_subzone[-which(sapply(bgc_pts_subzone, is.null))]
  names(bgc_pts_subzone) <- zone_names
  
  
  ## run model with all variables
  #model_bgc <- lapply(names(bgc_pts_subzone), function(xx) {
   
  xx <- names(bgc_pts_subzone[3])
    
    inmdata = bgc_pts_subzone[[xx]]
    out_name = names(bgc_pts_subzone[xx])
    
    # if(any(names(inmdata) %in% "x")) {
    # 
    # inmdata_all <- inmdata %>%
    #   dplyr::select(-c(x, y))
    # } else {
    #  inmdata_all <- inmdata
    # }
    inmdata_all <- inmdata
    #table(inmdata_all$target)
    #length(inmdata_all$target)
    
    outDir = file.path(paste(out_dir, out_name, sep = "/"))
   
       # test line for mapping    
    #inmdata_all <- inmdata_all %>%
    #  filter(slice != "1" )
    
    MU_count <- inmdata_all %>% dplyr::count(target) %>% filter(n >10) # use 10 for smo

    inmdata_all <-  inmdata_all %>% filter(target %in% MU_count$target)  %>%
       droplevels()
 
    # SBSmc2
    ds_ratio = 15#70 #100  #(0 - 100, Null = 100)
    sm_ratio = 0.5#0.3 #0.1 #0 - 1, 1 = complete smote 
            
            # ds_15_sm_0.5 : mapunit (spatial)
            # ds 70_sm_0.3 : ovreall (aspatial)
    
    # ESSFmc 
    #ds_ratio = 15 #90  #NA  #40  #(0 - 100, Null = 100)
   # sm_ratio = 0.2 #0.9 #0.2 #0.7 #0 - 1, 1 = complete smote
  
                # ds_15_sm_0.2 : mapunit (spatial)
                # ds 90_sm_0.9 : ovreall (aspatial)
    
      # ESSFmcw
      ds_ratio = 15
      sm_ratio = 0.8 #0.9
      
                # ds_20 : mapunit (spatial)
                # ds 29_sm_0.9 : ovreall (aspatial)
    
    model_gen_tidy(trDat = inmdata_all,
              target = "target",
              target2 = "target2",
              tid = "tid",
              ds_ratio = ds_ratio, # NA if no downsample
              sm_ratio = sm_ratio, # NA if no smoting 
              outDir = outDir,
              mname = mname,
              rseed = 456,
              infiles = infiles,
              mmu = mmu, 
              field_transect = NA)

     }) 
 
 }  

# Run entire study area, need to adjust for slices somehow?

#else if(str_detect(mname, 'all')) {
 
 # run the function using tidy models
   
#     covars <- mpts %>%
#      dplyr::select(-c("target", "target2", "slice", "tid", "bgc_cat")) 
#     
#     covars <- covars[complete.cases(covars[ ,1:length(covars)]),]
# 
# #  # remove correlated variables removing "target" column
# #    descrCor <- cor(covars)
# #    highlyCorDescr <- findCorrelation(descrCor, cutoff = 0.75, verbose = TRUE, names = TRUE)
# #    col.to.rm <- c(highlyCorDescr)
# 
# #    mpts <- mpts %>%
# #      dplyr::select(names(mpts)[!names(mpts) %in% col.to.rm])

  ###############################################################

# run the function using tidy models

   model_gen_tidy(trDat = mpts,
              target = "target",
              target2 = "target2",
              tid = "tid",
              outDir = out_dir,
              mname = mname,
              rseed = 456,
              infiles = infiles,
              mmu = mmu,
              field_transect = NA)
            
#   
#     smpts <- mpts[complete.cases(mpts[ , 5:length(mpts)]),] 
#     smpts <- smpts %>%
#       dplyr::select(-c(target2, tid, bgc_cat)) 
#   
#     MU_count <- smpts %>% dplyr::count(target) %>% filter(n > 20) 
#   
#     smpts <- smpts %>% filter(target %in% MU_count$target)  %>%
#       #drop_na() %>%
#       droplevels()
#  
#     
# # run function using mlr
# 
#     model_gen(smpts, 
#               "target", 
#               outDir = out_dir,
#               mname = mname, 
#               rseed = 456, 
#               infiles = infiles, 
#               mmu = mmu) 
#     
# # run ensemble model
#   # model_gen_esb(mpts, 
#               # "target", 
#               # outDir = file.path(
#               #   paste(out_dir, "enb_test", sep = "/")),
#               # mname = mname, 
#               # rseed = 456, 
#               # indata = indata, 
#               # mmu = mmu) 
 
     
     
   ## Otherwisew run model per BGC model  ####################################### 
     
   } 
  

    #### OR USE THE MLR coding option 
    ## # remove highly correlated variables
    ## descrCor <- cor(inmdata[,-(1:4)])
    ## highlyCorDescr <- findCorrelation(descrCor, cutoff = 0.75, verbose = FALSE, names = TRUE)
    ## col.to.rm <- c(highlyCorDescr)
    ## 
  ## inmdata = bgc_pts_subzone[[xx]] %>% 
    ##   dplyr::select(names(bgc_pts_subzone[[xx]])[!names(bgc_pts_subzone[[xx]]) %in% col.to.rm])
    
    # if(any(names(inmdata) %in% "x")) {
    # 
    # inmdata <- inmdata %>%
    #   dplyr::select(-c(x, y))
    # } else {
    #  inmdata <- inmdata
    # }
    # 
    #  inmdata <- inmdata %>% dplyr::select(-c(target2, tid, bgc_cat))
    # 
    #  inmdata <- inmdata %>%
    #    drop_na()
    # 
    # 
    #  MU_count <- inmdata %>% dplyr::count(target) %>% filter(n > 10)
    # 
    #  inmdata <-  inmdata %>% filter(target %in% MU_count$target)  %>%
    #     droplevels()
    # 
    # model_gen(inmdata,
    #           "target",
    #           outDir = outDir,
    #           infiles = infiles,
    #           mmu = mmu,
    #           mname = mname,
    #           rseed = 456)

# #    model_gen_esb(inmdata, "target", 
# #              outDir = outDir,  
# #    #         indata = indata, 
# #              mmu = mmu,
# #              mname = mname,
# #              infiles = infiles,
# #              rseed = 456)
#     
#   })
# 
# } 

```
 

 
Run the non_forest models 


```{r}

if(str_detect(mname, 'nf_')) {

  # filter for > 20 points 
  MU_count <- mpts %>% dplyr::count(target) %>% filter(n > 20) 
  mpts <- mpts %>% filter(target %in% MU_count$target)  %>%
    filter(target != "") %>%
    droplevels() 
  
  # check covars
  covars <- mpts %>% dplyr::select(-c("target" )) 
  covars <- covars[complete.cases(covars[ ,1:length(covars)]),]
 
  ###############################################################

# run the function using tidy models

  model_gen_tidy_nf(trDat = mpts,
              target = "target",
              outDir = out_dir,
              mname = mname,
              infiles = infiles,
              mmu = mmu)
            
#   
} 

```


 