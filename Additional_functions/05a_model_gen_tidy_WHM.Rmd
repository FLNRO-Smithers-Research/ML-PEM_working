---
title: "Accuracy Metrics for Deception Test Case"
output: html_document
params:
  outDir: "."
  trDat: trDat
  target: target
  target2: target2
  tid: tid
  rseed: NA
  infiles: infiles
  mmu: mmu
  mname: mname
  field_transect: field_transect
  
---

## This should be setup in a 4 deep loop
1. i iterations of models. I don't think we need to worry about point reduction. We are using the test transect metrics. 
        Effect of tp balancing, raw, smote .25/downsample 75, smote .5/down 50, smote .75/down 25, smote 1/down50
2. j iterations of BGC
3. k iterations of Train/Test slices leave one out (build 4 test 1. Further test of build 3 test 2, build 2 test 3, and build 1 test 4  for the ESSFmc and SBSmc2 only once inner loop optimization has occurred)
4. for each iteration build cv model and predict test -> return accuracy metrics 
Prepare directories

```{r session setup, tidy = TRUE, warning=FALSE}

AOI <- "Deception"
#AOI <- "BoundaryTSA"
# set up file structure
AOI_dir <- file.path("..", paste0(AOI,"_AOI"))
cov_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")
shapes_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers")
input_pnts_dir <- file.path(AOI_dir, "1_map_inputs", "trainingData")
out_dir <- file.path(AOI_dir, "3_maps_analysis","models")

# read in temp functions
source(here::here('_functions', 'model_gen.R'))
source(here::here('_functions', 'point_subsample.R'))
#source(here::here('_functions', 'predict_landscape.R'))
source(here::here('_functions', 'predict_map.R'))
source(here::here('_functions', 'model_gen_tidy.R'))

```

```{r read in lookup tables}
# read in map and model keys
map.key  <- read.csv(file.path(AOI_dir, "_MapUnitLegend", 
                                 paste0(AOI, "_MapUnitLegend.csv")), 
                       stringsAsFactor = FALSE)

# #read in the fuzzy index
fMat <- read.csv(file.path(AOI_dir, "_MapUnitLegend", 
                                  "fuzzy_matrix_basic.csv")) 
fMat <- data.table(fMat)

# get the map unit level 
model_param <- file.path(AOI_dir, "_MapUnitLegend", "models.xlsx")

# set up model parameters:  
mparam <- read_xlsx(model_param, "models") %>% filter(to_run == 1)
map_res <- mparam$resolution
data_res <- paste0("att_", map_res, "m")
mname <- paste0(mparam$model_name)
mrep <- mparam$model_rep

# check which catergory of model to be produced
mtype <- case_when(
  str_detect(mname, "for_nf")  ~ "forest_non_forest",
  str_detect(mname, "nf_") ~ "non_forest",
  str_detect(mname, "fore") ~ "forest"
)

mmu <- read_xlsx(model_param, "map_unit", skip = 2) %>%
   filter(!!sym(mparam$map_unit) == 1) %>%
  dplyr::select(legend_column)%>%
  pull

mmu <- case_when(
  mmu == "column_mu" ~ "MapUnit", 
  mmu == "column_ss" ~ "SiteSeries",
  mmu == "column_ass" ~ "Association",
  mmu == "column_cls" ~ "Class",
  mmu == "column_grp" ~ "Group",
  mmu == "column_typ" ~ "Type"
)
# get covariates
mcov <- read_xlsx(model_param, "covariates", skip = 2) %>%
  filter(!!sym(mparam$covariates) == 1) %>%
  dplyr::select(covariate)
mcol <- mcov$covariate

# get training point sets
mtpt <- read_xlsx(model_param, "training_pts", skip = 2) %>%
  filter(!!sym(mparam$training_pts) == 1) %>%
  dplyr::select(tp_code)%>%
  pull

```

```{r, echo=FALSE, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=FALSE,
                      warning = FALSE, message = FALSE,
                      results = 'show',
                      eval = TRUE)  ## flag eval = false for quick text edits

## Load the data and parameters as specified in the R script (model_gen_tidy.R)

trDat <- params$trDat
target <- params$target
target2 <- params$target2
tid <- params$tid
infiles<- params$infiles
mmu <- params$mmu
mname <- params$mname
field_transect <- params$field_transect
outDir <- params$outDir

library(data.table)
library(knitr)
library(cowplot)
library(tidymodels)
library(tidyverse)
library(themis)
library(ggplot2)
library(gridExtra)
library(janitor)
require(magicfor) 

## function to run accuracy metrics given a table with 2 columns representing target truth and predicted class
acc_metrix <- function(data){
acc_bal <- data %>% bal_accuracy(target, .pred_class)
 ppv <- data %>% ppv(target, .pred_class) # positive predictive value
 precision <- data %>% precision(target, .pred_class) # precision 
 recall <- data %>% recall(target, .pred_class) # recall
 kap <- data %>% kap(target, .pred_class) # kappa
 fmean <- data %>% f_meas(target, .pred_class) # f means
 mcc <- data %>%  mcc(target, .pred_class)
  sens <- data %>%  sens(target, .pred_class)
  spec <- data %>%  spec(target, .pred_class)
  acc <- data %>% accuracy(target, .pred_class)
  jind <- data %>% j_index(target, .pred_class) 
  ###some aspatial metrics
  #data <- test.pred
  aspatial_pred <- data  %>% dplyr::select(.pred_class) %>% group_by(.pred_class) %>% mutate(pred.ratio = n()) %>%ungroup() %>% distinct()
    aspatial_target <- data  %>% dplyr::select(target) %>% group_by(target) %>% mutate(targ.ratio = n())%>%ungroup() %>% distinct()    
      aspatial_sum <- full_join(aspatial_target, aspatial_pred, by = c("target" = ".pred_class")) %>% mutate_if(is.integer, funs(replace_na(., 0))) %>% rowwise() %>% mutate(Min = min(targ.ratio, pred.ratio))
      .estimate <- colSums(aspatial_sum[,4])/colSums(aspatial_sum[,2])
      .metric= "aspatial_acc"
      .estimator= "aspatial"
      aspatial_acc <- data.frame(.metric, .estimator, .estimate) 
      
    aspatial_sum <- aspatial_sum %>% mutate(unit_pos = Min/targ.ratio)
    mean_acc <- colMeans(aspatial_sum[5])
      .estimate <- mean_acc
      .metric= "aspatial_meanacc"
      .estimator= "aspatial"
      aspatial_meanacc <- data.frame(.metric, .estimator, .estimate) 
     final_metrics <- bind_rows (acc, mcc, aspatial_acc, aspatial_meanacc, acc_bal,ppv, precision, recall, kap, fmean, sens, spec, jind) %>% mutate_if(is.character, as.factor)
##______________ add alt-call metrics
##______________ add in fuzzy-call metrics     
##______________  add in metrics by map unit aspatial
     #unit_metrics
}

```

This model uses the following parameters: 

* **model reference:** `r params$mname` 
* **mapunit:** `r mmu`
* **training point set : **`r params$infiles`
* **model response and covariates: ** `r names(trDat)`


## Response variable: _`r target`_

The following training points and frequency of points were used in the model. 

```{r load data, echo = FALSE, include = TRUE}
 trDat <-  fread("D:/GitHub/PEM_Methods_DevX/Deception_AOI/1_map_inputs/trainingData/att_5m/Stage1_cleaned_5m_pts_data.csv")
table(trDat[, target])

MU.for <- map.key %>% dplyr::select(MapUnit, Type) %>% filter(Type == "For") %>% distinct()
MU.for <- MU.for$MapUnit
##tpts <- tpts %>% filter(target %in% MU.for)
MU_count2 <- trDat %>% dplyr::count(target)
  mpts <- trDat

  ## filter for pure calls only
  mpts$target2 <- mpts$target2 %>% na_if("")
  #mpts <- mpts %>% filter(is.na(target2))
  MU_count3 <- mpts %>% dplyr::count(target)
  # filter groups less than 20 pts
  #MU_count <- mpts %>% dplyr::count(target) %>% filter(n > 20) 

# calculate summary of raw training data set
trDat_sum <- trDat %>%
  dplyr::group_by(target) %>%
  summarise(freq = n()) %>%
  mutate(prop = round(freq/sum(freq),3))

ggplot(trDat, aes(target)) +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 90))

```


```{r, include = TRUE, echo = FALSE}
# format data for model by removing covars with NA values 

trDat_all <- trDat[complete.cases(trDat[ , 5:length(trDat)]),]
  
# create a subset of data by removing any variables not in model (required for model to run without error) 
# trDat <- trDat_all %>%
#     dplyr::select(-c(target2, tid, bgc_cat))
  
```

Data pre-processing includes the following steps: 

1: Set recipe

```{r prepare data, include = TRUE, echo = TRUE}
# 3: set up preparation of data sets 
BGCDat <- trDat_all %>% filter(bgc == "SBSmc2")  %>% dplyr::select(-bgc, -target2, -pt_id) %>% mutate_if(is.character, as.factor) %>% mutate_if(is.integer, as.numeric)
slices <- unique(BGCDat$slice) %>% droplevels()

              # load library
magic_for(print, silent = TRUE)

for (k in levels(slices)){
#test_train_MU_ratio <- foreach(k = iter, .combine = 'rbind') %do% {
#test_slice = k
#test_slice = "SBSmc2_1"
### separate train vs test based on 5-site slices
BGC_train <- BGCDat %>% filter(!slice %in% k) %>% dplyr::select(-slice) %>% droplevels()
BGC_test <- BGCDat %>% filter(slice %in% k) %>% dplyr::select(-slice)

trDat_sum <- BGC_train %>%
  dplyr::group_by(target) %>%
  summarise(freq = n()) %>%
  mutate(prop = round(freq/sum(freq),3))
trDat_sum$test_slice <- k
print (trDat_sum)

}
x <- magic_result_as_dataframe()
MU_ratios <- unnest(x)

ggplot(MU_ratios, aes(target, freq)) +
  geom_bar(stat = "identity") + 
  theme(axis.text.x = element_text(angle = 90))+
  facet_wrap (~test_slice)+
  ggtitle("Ratio of Map units when Test Slice is split")

# trDat_sum <- BGC_test %>%
#   dplyr::group_by(target) %>%
#   summarise(freq = n()) %>%
#   mutate(prop = round(freq/sum(freq),3))
# 
# ggplot(BGC_test, aes(target)) +
#   geom_bar() + 
#   theme(axis.text.x = element_text(angle = 90))+
# ggtitle("Testing Data")
```


```{r tidy parameters, include = TRUE, echo = TRUE}

uni_recipe <-
    recipe(target ~ ., data = BGC_train) %>%
    update_role(tid, new_role = "id variable") %>% 
    #step_corr(all_numeric()) %>%        # remove correlated covariates
    #step_dummy(all_nominal(),-all_outcomes()) %>%   
    #step_zv(all_numeric()) %>%          # remove values with no variance
    prep()
  summary(uni_recipe)
  
# pre-processed training point data check 

#table(training_dat$target)
    
ggplot(BGC_train , aes(target)) +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 90))+
  ggtitle("Training Set")
## note in RF as a tree based model it is not required to scale and normalize covariates and may have negative influence on the model performance 

balance_recipe <-  recipe(target ~ ., data =  BGC_train) %>%
    update_role(tid, new_role = "id variable") %>% 
    #step_corr(all_numeric()) %>%        # remove correlated covariates
    #step_dummy(all_nominal(),-all_outcomes()) %>%    
    # step_zv(all_numeric()) %>% # remove values with no variance
     step_downsample(target, under_ratio = 25) %>%
   step_smote(target, over_ratio = 1, neighbors = 2) %>% 
    prep()

BGC_train2 <- balance_recipe  %>% juice()#BGC_train2 <- PEM_recipe %>% prep() %>% juice()
ggplot(BGC_train2 , aes(target)) +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 90))+
  ggtitle("Up/downsampled set")
```

2: Optional:  Perform model hyperparameter tuning (optional)   

Select model and tune the parameters, note this is time consuming. Model tuning is performed the resampled data by splitting each fold into analysis and assessment components. For each candidate model we should retune the model to select the best fit. However due to the intense computation and time we will tune the model only once for each candidate model and check results with full 10 x 5 CV tuning. Tuning outputs are inspected to assess the best hyperparameters for the given model based on a chosen meaure of accuracy (ie accuracy, j_index, roc). Once the hyperparamters are selected we update the model and apply this to the entire resampled dataset.  Two methods are available





3: Set up cross validation

```{r set-up cross validation, eval = TRUE}
## set up cross validation for parameter tuning data sets # note cv best practice is 10 fold x 5 rep

set.seed(345)
pem_cvfold <- group_vfold_cv(BGC_train, 
                          v = 10, ### need to build a check for number of tids available to automatically reduce this number where necessary
                          repeats = 5, 
                          group = tid,
                          strata = target)

summary(pem_cvfold)
```


4: Run CV metrics with updated hyperparameters

```{r run model with tuned hyperparameters, echo = TRUE}
#define the model with parameters from tune chunk

randf_spec <- rand_forest(mtry = 10, min_n = 2, trees = 200) %>% ## trees = 200 is approximately good metrics improve by 1% going 100 -> 200 but go down at higher ntrees
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity", verbose = TRUE) #or "permutations

pem_workflow <- workflow() %>%
    add_recipe(balance_recipe) %>%
    add_model(randf_spec)

#cv_metric_set <- metric_set(accuracy, roc_auc, j_index, sens, spec)

set.seed(4556)
doParallel::registerDoParallel() # note when smoting with fit_resample you cant use parrallel process or will cause error

cv_results <- fit_resamples(
  pem_workflow, 
  resamples = pem_cvfold, 
 # metrics = cv_metric_set, 
  control = control_resamples(save_pred = TRUE))

# collect metrics  
cv_metrics <- cv_results  %>% collect_metrics(summarize = FALSE)
cv_metrics_sum <- cv_results %>% collect_metrics()

# collect predictions
cv_pred <- cv_results %>% collect_predictions(summarize = FALSE)
cv_pred_sum <- cv_results %>% collect_predictions(summarize = TRUE)

```

## CV model accuracy metrics

```{r run final model metrics, include = TRUE, echo = FALSE, error=TRUE}
final_metrics <- acc_metrix(cv_pred_sum)
kable(final_metrics)

# confidence interval based on average prediction confus

conf_matrix <- cv_pred_sum %>% 
   conf_mat(target, .pred_class) %>%
   pluck(1) %>%
   as_tibble() %>%
   ggplot(aes(Prediction, Truth, alpha = n)) +
   geom_tile(show.legend = FALSE) +
   geom_text(aes(label = n), colour = "black", alpha = 1, size = 3) + 
   theme(axis.text.x = element_text(angle = 90))
    
 conf_matrix

```

## now build final train model and predict test data and compare acc_metrix to cv results

```{r fit non-cv model fpr comparison, error=TRUE}
randf_spec <- rand_forest(mtry = 10, min_n = 2, trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity") #or "permutations

pem_workflow <- workflow() %>%
    add_recipe(balance_recipe) %>%
    add_model(randf_spec)

PEM_rf1 <- fit(
  pem_workflow, 
  BGC_train)

######### Predict Test
test_target <-as.data.frame(BGC_test$target) %>% rename(target = 1)
test.pred <-  predict(PEM_rf1, BGC_test)
test.pred <- cbind(test_target, test.pred) %>% mutate_if(is.character, as.factor)
# levels(train.pred$target)

###harmonize levels
targ.lev <- levels(test.pred$target); pred.lev <- levels(test.pred$.pred_class)
levs <- c(targ.lev, pred.lev) %>% unique()
test.pred$target <- factor(test.pred$target, levels = levs)
test.pred$.pred_class <- factor(test.pred$.pred_class, levels = levs)
# 
# 
# train.acc <- acc_metrix(train.pred) %>% rename(train = .estimate)
test.acc <- acc_metrix(test.pred) %>% dplyr::select(.estimate) %>% rename(test = 1)

  test.pred %>% 
   conf_mat(target, .pred_class) %>%
   pluck(1) %>%
   as_tibble() %>%
   ggplot(aes(Prediction, Truth, alpha = n)) +
   geom_tile(show.legend = FALSE) +
   geom_text(aes(label = n), colour = "black", alpha = 1, size = 3) + 
   theme(axis.text.x = element_text(angle = 90)) 
## compare cv stats to test stats  
  acc.compare <- cbind(final_metrics, test.acc)
kable(acc.compare)
```

```{r compiled bootstrap accuracy metrics}

```




```{r fit final model with all data, echo = FALSE, error=TRUE}
##Apply tuned model to all the data for predictions

best_workflow <- workflow() %>%
    add_recipe(balance_recipe) %>%
    add_model(randf_spec)

model_final <- fit(best_workflow, data = BGCDat)

final_fit <- pull_workflow_fit(model_final)# %>%pull(.predictions)
variables <- final_fit %>% vip(num_feature = 20)
variables
oob  <- round(model_final$fit$fit$fit$prediction.error, 3)

saveRDS(model_final , file = paste(paste0(".", outDir), "tidy_model.rds", sep = "/"))

```

#
```{r define models with tuning parameter option, echo = FALSE, eval= FALSE}
# define model 
# 
# randf_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
# #randf_spec <- rand_forest(mtry = 20, min_n = 2, trees = 1000) %>%
#   set_mode("classification") %>%
#   set_engine("ranger", importance = "impurity") #or "permutations
# 
# # mtry = how many leaves to you sample at each tree
# # trees = number of trees, just need enough
# # min_n = how many data points need to be in node before stop splitting
# 
# pem_workflow <- workflow() %>%
#     add_recipe(uni_recipe) %>%
#     add_model(randf_spec)
# 
# cv_metrics <- metric_set(accuracy, roc_auc, j_index)
# 
# set.seed(345)
# pem_cvfold <- vfold_cv(trDat, 
#                           v = 10, 
#                           repeats = 3, 
#                           strata = target)
# 
# 
# # Two methods are available for tuning; 1) basic grid and 2) regular grid. 
# # For Random Forest we are using regular grid tune to assess hyperparameters. 
# 
# # Tune the model
# # ##https://www.youtube.com/watch?v=ts5bRZ7pRKQ
# # https://www.tidymodels.org/start/case-study/
# # http://www.rebeccabarter.com/blog/2020-03-25_machine_learning/
# #install.packages("tictoc")
# 
# library(tictoc)
# 
# # # look at more bound options (ie c(2, 6, 10))
# ranger_tune_detail <-
#   grid_regular(
#     mtry(range = c(2, 40)),
#     min_n(range = c(2, 10)),
#     levels = 5)
# 
# # # re-run the tuning with the explicit parameter sets
# tic()
# set.seed(4556)
# doParallel::registerDoParallel()
# ranger_tune <-
#   tune_grid(pem_workflow,
#             resamples = pem_cvfold,
#             #metrics = cv_metrics,
#             grid = ranger_tune_detail)
# toc()
# 
# saveRDS(ranger_tune, file = paste(paste0(".", outDir), "parameter_tune_results.rds", sep = "/"))
# 
# # explore ranger tune output
# ranger_tune %>%
#   dplyr::select(.metrics) %>%
#   unnest(cols = c(.metrics))
# 
# # explore results of tuning models note different for type of model selected
# select_best(ranger_tune, metric = "accuracy")
# select_best(ranger_tune, metric = "roc_auc")
# #select_best(ranger_tune, metric = "j_index")
# 
# autoplot(ranger_tune)
# 
# # Plot the impact of different values for the hyperparamters. note these are randomly selected for the number of grids specified (ie. grid = 20).
# # This provides an overview of the impact of ech tune paramter. Note this provides an overview as we dont know what min_n was each mtry etc...
# # this can be used to set up the tune grid paramter
# 
# ranger_tune %>%
#   collect_metrics() %>%
#   filter(.metric == "roc_auc") %>%
#   dplyr::select(mean, min_n, mtry) %>%
#   pivot_longer(min_n:mtry,
#                values_to = "value",
#                names_to = "parameter") %>%
#   ggplot(aes(value, mean, colour = parameter)) +
#   geom_point(show.legend = FALSE)+
#   facet_wrap(~parameter)
# 
# 
# # for the 30 m standard pt # minimal change in mtry after 5 - 10 
# # mtry = 10, min_n = 2

```

# Internal transect overlay accuracy determination (TOAD). 

In addition to internal machine learning metrics we assessed the predicted map accuracy using a bootstrap method by sites (transect pair). This process withholds one site and uses the remaining data to build a model and predict the map surface. The predicted map can then be compared to raw data for the with-held transect (at the resolution of the model) with-held site data (750m x 2 transects) to determine the map accuracy. This process is repeated for the total number of sites to provide a measure of the range in success at the map level.

Map accuracy by map unit is a measure of how accurate the prediction is on a mapunit. Note currently success is only limited to field data relating to the modeled sub units. i.e. Non-forest field calls are grouped as "other" and not included in success rate for aspatial and spatial comparisons. The size of the points indicates the proportion of points on the transect (ie how much was observed on the landscape).

```{r spatial map accuracy test, echo = FALSE, eval = TRUE}
# 
# # read in the all points transect data for the correct resolution and edit names
# # this will be compared with the predicted surface. 
# 
# field_transect <- field_transect %>%
#       dplyr::rename(target = mapunit1, 
#                     target2 = mapunit2) 
# 
# # get unique names and number of unique sites for the training data     
# trID <- unique(trDat_all$tid) %>% as.character()
# trID <- trID[!is.na(trID)]
# 
# ntrID <- length(trID)
# 
# # subset data by removing columns not used for modelling
# 
# trAll <- trDat_all %>%
#    dplyr::select(-c(target2, bgc_cat))
# 
# 
# ##TO FIX =- change mapunit to target ?????
# 
# #trans_units <- unique(field_transect$target)
#    
# # iterate through each of the sites 
# bsRes <- foreach(it = trID, .combine = rbind) %do% {
# 
#       #it = trID[9] # testing line 
#       #print (paste0("currently processing ", it))
#       testID = it
#        
#       # get training dataset by removing held-out transect
#       trainDat <- trAll %>% filter(tid != testID)
#       trainDat <- data.table(trainDat)
#       trainDat[,`:=`(tid = NULL)]
#       numTr <- trainDat[,.(Num = .N), by = .(target)]
#     
#       trainClean <- foreach(unit = numTr$target, .combine = rbind) %do% {
#         dat <- trainDat[target == unit,]
#         dat
#       }
#       
#       trainClean <- unique(trainClean)
#       trainClean[,target := as.factor(as.character(target))]
#       
#       ##create model using cleaned transect data
#       set.seed(1234)
#       
#       bs_model <- fit(pem_workflow, data = trainClean)
#       
#       mod <- pull_workflow_fit(bs_model)$fit
#         
#       # subset the entire point data set based on transect ID for site of interest
#       testDat <- field_transect %>% dplyr::filter(tid == as.factor(as.character(testID)))
#       
#      #testDat <- field_transect %>% dplyr::filter(tid %in% "essfmcw")
#       
#       # create a subset with calls and XY values to add after predict
#       testDat_id <- testDat %>% 
#         dplyr::select(target, target2) %>%
#         cbind(st_coordinates(.)) %>%
#         st_drop_geometry(.)
#       
#       # create small table with columns for model only 
#       testDat <- testDat %>%
#         dplyr::select(any_of(names(trainClean))) %>%
#         dplyr::select(-target) %>%
#         st_drop_geometry()
# 
#       # predict onto full transect of hold-out and convert from probability matrix        to single response 
#       Pred_prop <- predict(mod, testDat)$predictions
#       Pred <- colnames(Pred_prop)[apply(Pred_prop, 1, which.max)]
#       
#       # combine prediction and XY values to all points data set
#       testDat <- cbind(testDat_id , Pred)
#       testDat <- testDat %>%
#         mutate(target = ifelse(target == " ", NA, target))
# 
#       length(testDat$target)
#       
# # Important point!
# # remove any values where non_forest in found on the ground (primary or secondary)
#       
#       testDat <- testDat %>%
#         #rowwise() %>%
#         #mutate(nf_to_drop = ifelse(str_detect(target, "_")|str_detect(target2,"_"), 1,0))
#       mutate(nf_to_drop = ifelse(str_detect(target, "_"), 1,0))
#         
# #     xxx <- testDat %>% 
# #        dplyr::select(target, target2, Pred,nf_to_drop) %>%
# #         distinct()
# #      xxx
# 
#       testDat <- testDat %>% 
#         filter(nf_to_drop == 1) %>%
#         dplyr::select(-nf_to_drop)
#       
#       length(testDat$target)
#       
#       # calculate metrics 
#     
#       # 1) aspatial metrics 
#       aspatial <- testDat %>%
#         dplyr::select(target) %>%
#         group_by(target) %>%
#         mutate(trans.total = n()) %>%
#         distinct()
#       
#       #filter out non_forest 
#      #aspatial<- aspatial %>%
#     #    dplyr::filter(str_detect(target, "_", negate = FALSE))
#       
#       aspatial_map <- testDat   %>% 
#         dplyr::select(Pred)%>%
#         group_by(Pred) %>%
#         mutate(map.total = n(),
#                target = Pred) %>%
#         ungroup()%>%
#         dplyr::select(-Pred)%>%
#         distinct()
#       
#       aspatial <- full_join(aspatial, aspatial_map, by = "target") %>%
#         mutate(It = it, 
#                across(where(is.numeric), ~ replace_na(.,0)))
#     
#       # generate spatially explicit results for primary and prime/alternate
#       xx <- testDat %>%
#          tabyl(target, Pred)
#       
#       xy <- pivot_longer(xx, cols = !target) 
#   
#       spat_p <- xy %>%
#         filter(target == name) %>%
#         mutate(spat_p = value ) %>%
#         dplyr::select(target, spat_p)
#     
#       # extract spatial secondary call match 
#       spat_pa <- testDat %>%
#         filter(!is.na(target2)) %>%
#         filter(target != Pred)
# 
#        if(nrow(spat_pa) == 0){
#          
#          spat_pa <- spat_p %>%
#            mutate(spat_pa = 0 ) %>%
#            dplyr::select(-spat_p)
#       #  aspatial <- left_join(aspatial, spat_p, by = "target") 
#           } else {
#          
#       spat_pa <- spat_pa %>%
#         tabyl(target2, Pred) %>%
#         pivot_longer(cols = !target2) %>%
#         filter(target2 == name) %>%
#         mutate(target = target2, 
#                spat_pa = value) %>%
#         dplyr::select(target, spat_pa)
#           }
#       
#        aspatial <- left_join(aspatial, spat_p, by = "target") %>%
#          left_join(spat_pa, by = "target")
#     
#        # generate the primary fuzzy calls: 
#        spat_fp_df <- xy %>%
#          left_join(fMat, by = c("target" = "target", "name" = "Pred")) %>%
#          rowwise() %>%
#          mutate(spat_fpt = fVal * value) %>%
#          group_by(target) %>%
#          mutate(spat_fp = sum(spat_fpt)) %>%
#          dplyr::select(target, spat_fp) %>%
#          distinct()
#     
#        aspatial <- left_join(aspatial,  spat_fp_df , by = "target") 
# 
#        # generate fuzzy secondary calls : 
#       spat_fpa_df <- testDat %>%
#         dplyr::select(-X,-Y) %>%
#         left_join(fMat, by = c("target" = "target", "Pred" = "Pred")) %>%
#         left_join(fMat, by = c("target2" = "target", "Pred" = "Pred")) %>%
#         mutate(across(where(is.numeric), ~ replace_na(.,0))) %>%
#         rowwise() %>%
#         mutate(targetMax = ifelse(fVal.x >= fVal.y , target, target2)) %>%
#         dplyr::select(targetMax, Pred) %>%
#         tabyl(targetMax, Pred) %>%
#         pivot_longer(cols = !targetMax) %>%
#         left_join(fMat, by = c("targetMax" = "target", "name" = "Pred")) %>%
#          rowwise() %>%
#          mutate(spat_fpat = fVal * value) %>%
#          group_by(targetMax) %>%
#          mutate(spat_fpa = sum(spat_fpat)) %>%
#          dplyr::select(target = targetMax, spat_fpa) %>%
#          distinct()
#       
#        aspatial <- left_join(aspatial,  spat_fpa_df , by = "target") %>%
#          mutate(spat_pa = sum(spat_pa, spat_p, na.rm = TRUE))
#       
#        
#       # calculate accuracy and J statistic
#       testDat$target <- factor(testDat$target, levels = unique(c(levels(testDat$target), testDat$Pred)))
#       testDat$Pred <- factor(testDat$Pred, levels = levels(testDat$target))
#     
#       sen <- sens(testDat, target, Pred)
#       spec <- spec(testDat, target, Pred)
#       accuracy <- accuracy(testDat, target, Pred)
#       j_index <- j_index(testDat, target, Pred) 
#       mcc_val <- mcc(testDat, target, Pred)
#       bal_accuracy <- bal_accuracy(testDat, target, Pred)
#       f_means <- f_meas(testDat, target, Pred)
#       precision <- precision(testDat, target, Pred) # precision 
#       recall <- recall(testDat, target, Pred) # recall
#       kap <- kap(testDat, target, Pred) # kappa
#       
#       dout <- rbind(sen, spec, accuracy, j_index, mcc_val, bal_accuracy, f_means, 
#                     precision, recall, kap) %>%
#         dplyr::select(-.estimator) %>%
#         pivot_wider(names_from = .metric, values_from = .estimate)
#         
#       aspatial = cbind(aspatial, dout) 
#        
# } 
# 
# ## Error check
# #sum(bsRes$trans.total, na.rm = TRUE)
# #sum(bsRes$map.total, na.rm = TRUE)
# 
# # format bsRes
# bsRes <- bsRes %>%
#     ungroup() %>%
#     mutate(across(where(is.numeric), ~ replace_na(.,0)))
# 
# 
# ```
# 
#     
# ```{r, eval = FALSE, include = FALSE}
# # testing using slices rather than individual sites 
# 
# 
# # get unique slice names and number for iteration 
# slID <-  unique(gsub("\\..*", "", unique(trDat_all$tid)))
# slID <- slID[!is.na(slID)]
# nslID <- length(slID)
# 
# 
# # iterate through each of the sites 
# bsRes <- foreach(it = slID , .combine = rbind) %do% {
# 
#       it = slID[1] # testing line 
#       #print (paste0("currently processing ", it))
#       testID = it
#        
#       # get training dataset by removing held-out transect
#       trainDat <- trAll %>% filter(tid != testID)
#       
#       # changed this line 
#       trainDat <- trAll %>%
#        dplyr::filter(str_detect(tid, testID, negate = TRUE )) %>%
#        droplevels()
#       
#       
#       
#       
#       
#       trainDat <- data.table(trainDat)
#       trainDat[,`:=`(tid = NULL)]
#       numTr <- trainDat[,.(Num = .N), by = .(target)]
#     
#       trainClean <- foreach(unit = numTr$target, .combine = rbind) %do% {
#         dat <- trainDat[target == unit,]
#         dat
#       }
#       
#       trainClean <- unique(trainClean)
#       trainClean[,target := as.factor(as.character(target))]
#       
#       ##create model using cleaned transect data
#       set.seed(1234)
#       bs_model <- fit(model_final, data = trainClean)
#       
#       mod <- pull_workflow_fit(bs_model)$fit
#         
#       # subset the entire point data set based on transect ID for site of interest
#       testDat <- field_transect %>% dplyr::filter(tid == as.factor(as.character(testID)))
#       
#      #testDat <- field_transect %>% dplyr::filter(tid %in% "essfmcw")
#       
#       # create a subset with calls and XY values to add after predict
#       testDat_id <- testDat %>% 
#         dplyr::select(target, target2) %>%
#         cbind(st_coordinates(.)) %>%
#         st_drop_geometry(.)
#       
#       # create small table with columns for model only 
#       testDat <- testDat %>%
#         dplyr::select(any_of(names(trainClean))) %>%
#         dplyr::select(-target) %>%
#         st_drop_geometry()
# 
#       # predict onto full transect of hold-out and convert from probability matrix        to single response 
#       Pred_prop <- predict(mod, testDat)$predictions
#       Pred <- colnames(Pred_prop)[apply(Pred_prop, 1, which.max)]
#       
#       # combine prediction and XY values to all points data set
#       testDat <- cbind(testDat_id , Pred)
#       testDat <- testDat %>%
#         mutate(target = ifelse(target == " ", NA, target))
# 
#       length(testDat$target)
#       
# # Important point!
# # remove any values where non_forest in found on the ground (primary or secondary)
#       
#       testDat <- testDat %>%
#         #rowwise() %>%
#         #mutate(nf_to_drop = ifelse(str_detect(target, "_")|str_detect(target2,"_"), 1,0))
#       mutate(nf_to_drop = ifelse(str_detect(target, "_"), 1,0))
#         
# #     xxx <- testDat %>% 
# #        dplyr::select(target, target2, Pred,nf_to_drop) %>%
# #         distinct()
# #      xxx
# 
#       testDat <- testDat %>% 
#         filter(nf_to_drop == 1) %>%
#         dplyr::select(-nf_to_drop)
#       
#       length(testDat$target)
#       
#       # calculate metrics 
#     
#       # 1) aspatial metrics 
#       aspatial <- testDat %>%
#         dplyr::select(target) %>%
#         group_by(target) %>%
#         mutate(trans.total = n()) %>%
#         distinct()
#       
#       #filter out non_forest 
#      aspatial<- aspatial %>%
#         dplyr::filter(str_detect(target, "_", negate = FALSE))
#       
#       aspatial_map <- testDat   %>% 
#         dplyr::select(Pred)%>%
#         group_by(Pred) %>%
#         mutate(map.total = n(),
#                target = Pred) %>%
#         ungroup()%>%
#         dplyr::select(-Pred)%>%
#         distinct()
#       
#       aspatial <- full_join(aspatial, aspatial_map, by = "target") %>%
#         mutate(It = it, 
#                across(where(is.numeric), ~ replace_na(.,0)))
#     
#       # generate spatially explicit results for primary and prime/alternate
#       xx <- testDat %>%
#          tabyl(target, Pred)
#       
#       xy <- pivot_longer(xx, cols = !target) 
#   
#       spat_p <- xy %>%
#         filter(target == name) %>%
#         mutate(spat_p = value ) %>%
#         dplyr::select(target, spat_p)
#     
#       # extract spatial secondary call match 
#       spat_pa <- testDat %>%
#         filter(!is.na(target2)) %>%
#         filter(target != Pred)
# 
#        if(nrow(spat_pa) == 0){
#          
#          spat_pa <- spat_p %>%
#            mutate(spat_pa = 0 ) %>%
#            dplyr::select(-spat_p)
#       #  aspatial <- left_join(aspatial, spat_p, by = "target") 
#           } else {
#          
#       spat_pa <- spat_pa %>%
#         tabyl(target2, Pred) %>%
#         pivot_longer(cols = !target2) %>%
#         filter(target2 == name) %>%
#         mutate(target = target2, 
#                spat_pa = value) %>%
#         dplyr::select(target, spat_pa)
#           }
#       
#        aspatial <- left_join(aspatial, spat_p, by = "target") %>%
#          left_join(spat_pa, by = "target")
#     
#        # generate the primary fuzzy calls: 
#        spat_fp_df <- xy %>%
#          left_join(fMat, by = c("target" = "target", "name" = "Pred")) %>%
#          rowwise() %>%
#          mutate(spat_fpt = fVal * value) %>%
#          group_by(target) %>%
#          mutate(spat_fp = sum(spat_fpt)) %>%
#          dplyr::select(target, spat_fp) %>%
#          distinct()
#     
#        aspatial <- left_join(aspatial,  spat_fp_df , by = "target") 
# 
#        # generate fuzzy secondary calls : 
#       spat_fpa_df <- testDat %>%
#         dplyr::select(-X,-Y) %>%
#         left_join(fMat, by = c("target" = "target", "Pred" = "Pred")) %>%
#         left_join(fMat, by = c("target2" = "target", "Pred" = "Pred")) %>%
#         mutate(across(where(is.numeric), ~ replace_na(.,0))) %>%
#         rowwise() %>%
#         mutate(targetMax = ifelse(fVal.x >= fVal.y , target, target2)) %>%
#         dplyr::select(targetMax, Pred) %>%
#         tabyl(targetMax, Pred) %>%
#         pivot_longer(cols = !targetMax) %>%
#         left_join(fMat, by = c("targetMax" = "target", "name" = "Pred")) %>%
#          rowwise() %>%
#          mutate(spat_fpat = fVal * value) %>%
#          group_by(targetMax) %>%
#          mutate(spat_fpa = sum(spat_fpat)) %>%
#          dplyr::select(target = targetMax, spat_fpa) %>%
#          distinct()
#       
#        aspatial <- left_join(aspatial,  spat_fpa_df , by = "target") %>%
#          mutate(spat_pa = sum(spat_pa, spat_p, na.rm = TRUE))
#       
#        
#       # calculate accuracy and J statistic
#       testDat$target <- factor(testDat$target, levels = unique(c(levels(testDat$target), testDat$Pred)))
#       testDat$Pred <- factor(testDat$Pred, levels = levels(testDat$target))
#     
#       sen <- sens(testDat, target, Pred)
#       spec <- spec(testDat, target, Pred)
#       accuracy <- accuracy(testDat, target, Pred)
#       j_index <- j_index(testDat, target, Pred) 
#       mcc_val <- mcc(testDat, target, Pred)
#       bal_accuracy <- bal_accuracy(testDat, target, Pred)
#       f_means <- f_meas(testDat, target, Pred)
#       precision <- precision(testDat, target, Pred) # precision 
#       recall <- recall(testDat, target, Pred) # recall
#       kap <- kap(testDat, target, Pred) # kappa
#       
#       dout <- rbind(sen, spec, accuracy, j_index, mcc_val, bal_accuracy, f_means, 
#                     precision, recall, kap) %>%
#         dplyr::select(-.estimator) %>%
#         pivot_wider(names_from = .metric, values_from = .estimate)
#         
#       aspatial = cbind(aspatial, dout) 
#        
# } 
# 
# ## Error check
# #sum(bsRes$trans.total, na.rm = TRUE)
# #sum(bsRes$map.total, na.rm = TRUE)
# 
# # format bsRes
# bsRes <- bsRes %>%
#     ungroup() %>%
#     mutate(across(where(is.numeric), ~ replace_na(.,0)))
# 
# 
# ```
#     
#     
#     
# ## Overall accuracy. 
# 
# The overall map accuracy was calculated but determining the percent correct for each map unit by comparing transect data (held-out from the model) with the predicted map surface. This process was conducted for each site (pair of transects) to produce uncertainty metrics. Note this is suceptible to outliers and unbalanced transects. Note: as each model was limited to forest or specific bgc 
# 
# ### Types of accuracy 
# Several types of accuracy measures were calculated;
# 
# 1) aspatial: this is equivalent to traditional AA where proportion of map units are compared
# 
# 2) spatial (spat_p): this compares spatial equivalents for the primary call for each pixal/point predicted.
# 
# 3) spatial primary/alt calls (spat_pa). This assigns a value if the alternate call matches the predicted call. 
# 
# 4) fuzzy spatially explicit accuracy: we tested an alternate accuracy measure (spat_fp) to account for calls which were similar (on the edatopic position) to the correct calls. In this case scores could be awarded for on a sliding scale from 1 (Correct) to 0 (no where close) with partial credit assigned to closely related mapunits. Note this requires a matrix which specifies the similarity between all combinations of possible calls. This was also calculated for primary and alternate calls (spat_fpa)
# 
# 
# ```{r, overall accuracy with confidence intervals, echo = FALSE, eval = TRUE}
# 
# bsRes_all <- bsRes %>% 
#   group_by(It) %>%
#   mutate(trans_sum = sum(trans.total)) %>%
#   rowwise() %>%
#   mutate(aspat_p = min((trans.total/trans_sum),(map.total/trans.total))*100) %>%
#   group_by(It) %>%
#   mutate(aspat_p_tot = sum(aspat_p),
#          spat_p_tot = sum(spat_p/sum(trans.total))*100,
#          spat_pa_tot = sum(spat_pa/sum(trans.total))*100,
#          spat_fp_tot = sum(spat_fp/sum(trans.total))*100,
#          spat_fpa_tot = sum(spat_fpa/sum(trans.total))*100,
#          spat_p_sens = sens * 100,
#          spat_p_spec = spec * 100,
#          spat_p_accuracy = accuracy * 100,
#          spat_p_j_index = j_index * 100,
#          spat_p_bal_accuracy = bal_accuracy * 100,
#          spat_p_mcc = mcc *100,
#          spat_p_recall = recall * 100,
#          spat_p_f_meas = f_meas * 100,
#          spat_p_precision = precision * 100,
#          spat_p_kap = kap * 100 
#          ) %>%
#   dplyr::select(c(It, aspat_p_tot, spat_p_tot, spat_pa_tot,spat_fp_tot, spat_fpa_tot,spat_p_sens:spat_p_kap )) %>%
#    pivot_longer(cols = where(is.numeric), names_to = "accuracy_type", values_to = "value") %>%
#   distinct()
# 
# 
# p2 <- ggplot(aes(y = value, x = accuracy_type), data = bsRes_all) + 
#    geom_boxplot() + 
#    geom_jitter(position=position_jitter(width=.1), colour = "grey", alpha = 0.8) + 
#    geom_hline(yintercept = 65,linetype ="dashed", color = "red") + 
#    ggtitle("Overall accuracy (median + quartiles)") + 
#    theme(axis.text.x = element_text(angle = 90)) +
#    xlab("Mapunit") + ylab("Accuracy") + 
#    ylim(-0.05, 100) 
# 
# p2
# 
# ```
# 
# ## Accuracy per mapunit
# 
# We can compare map unit accuracy levels to assess under or acceptable performance per map units. 
# 
# ```{r generate overall mapunit, echo = FALSE, eval = TRUE}
# 
# # need to drop the accuracy measures as these are base on entire confusion matrix not mapunit
# 
# mapunit_tot <- bsRes %>% 
#   group_by(target) %>%
#   summarise(across(.cols = where(is.numeric), ~ sum(.x, na.rm = TRUE))) %>%
#   ungroup() %>%
#   dplyr::select(-c(sens:kap))
#   
# mapunit_pc <- mapunit_tot %>%
#   rowwise() %>%
#   summarise(across(.cols = starts_with("spat"), ~ (.x)/trans.total*100)) %>%
#   bind_cols(target = mapunit_tot$target) %>%
#   pivot_longer(cols = where(is.numeric), names_to = "accuracy_type", values_to = "value") %>%
#   filter(target %in%  target[str_detect(target, "_")]) 
# 
# # for primary outputs only
# mapunit_pc_p <- mapunit_pc %>%
#   filter(accuracy_type == "spat_p")
# 
# p2 <- ggplot(aes(y = value, x = target), data = mapunit_pc_p) + 
#    geom_bar(stat = "identity") + 
#    geom_hline(yintercept = 65,linetype ="dashed", color = "red") + 
#    ggtitle("Unweighted spatial (primary) accuracy per mapunit") + 
#    theme(axis.text.x = element_text(angle = 90)) +
#    xlab("Mapunit") + ylab("Accuracy") + 
#    ylim(-0.05, 100)
# 
# p2
# 
# 
# # export out the raw values to compare between models ; add weighted value 
# # all output values
# # 
# # p3 <- ggplot(aes(y = value, x = target, fill = accuracy_type ), data = mapunit_pc) + 
# #    geom_bar(stat = "identity", position=position_dodge()) + 
# #    geom_hline(yintercept = 65,linetype ="dashed", color = "red") + 
# #    ggtitle("Unweighted accuracy per mapunit (per site)") + 
# #    theme(axis.text.x = element_text(angle = 90)) +
# #    xlab("Mapunit") + ylab("Accuracy") + 
# #    ylim(-0.05, 100) + 
# #    scale_fill_brewer(palette="RdYlGn") 
# # 
# # p3
# 
# ```
# 
# 
# ```{r map unit accuracy, echo = FALSE, fig.height = 5, fig.width = 10, eval = TRUE}
# # option 2: calculate overall accuracy based on iterations 
# 
# bsRes_site <- bsRes %>% 
#   #filter(It == "SBSmc2_5.2_22") %>%
#   rowwise() %>%
#   mutate(spat_p_pc = spat_p/trans.total * 100, 
#          spat_pa_pc = spat_pa/trans.total * 100,
#          spat_fp_pc = spat_fp/trans.total * 100, 
#          spat_fpa_pc = spat_fpa/trans.total * 100,
#          aspat_p_pc = min(trans.total, map.total)/trans.total * 100)
#  
# bsRes_site_totals <- bsRes_site %>%
#   dplyr::select(c(target, It, trans.total)) 
# 
# bsRes_site <-  bsRes_site %>%
#  pivot_longer(cols = where(is.numeric), names_to = "accuracy_type", values_to = "value") %>%
# #  filter(accuracy_type %in% c("spat_p_pc", "spat_pa_pc", "spat_fp_pc", "spat_fpa_pc", "aspat_p_pc"))  %>%
#   filter(accuracy_type %in% c("spat_p_pc", "aspat_p_pc")) %>%
#   filter(target %in%  target[str_detect(target, "_")]) %>%
#   left_join(bsRes_site_totals)
#        
#   
# p3 <- ggplot(aes(y = value, x = target ), data = bsRes_site) + 
#    geom_boxplot() + #stat = "identity") + 
#    facet_wrap(~accuracy_type) + 
#    geom_jitter(aes(size = trans.total), position=position_jitter(width=.2), colour = "grey", alpha = 0.5) + 
#    geom_hline(yintercept = 65,linetype ="dashed", color = "red") + 
#    ggtitle("Unweighted accuracy per mapunit (per site)") + 
#    theme(axis.text.x = element_text(angle = 90)) +
#    xlab("Mapunit") + ylab("Accuracy") + 
#    ylim(-0.05, 100) 
# 
# 
# p3
# ```
# **note**: size of points represent the number of points 
# 
# ```{r map unit accuracy 2, echo = FALSE, fig.height = 10, fig.width = 15}
# # 
# # p4 <- ggplot(aes(y = value, x = target ), data = bsRes_site) + 
# #    geom_boxplot() + #stat = "identity") + 
# #    facet_wrap(~accuracy_type) + 
# #    geom_jitter(position=position_jitter(width=.2), colour = "grey", alpha = 0.5) + 
# #    geom_hline(yintercept = 65,linetype ="dashed", color = "red") + 
# #    ggtitle("Unweighted accuracy per mapunit (per site)") + 
# #    theme(axis.text.x = element_text(angle = 90)) +
# #    xlab("Mapunit") + ylab("Accuracy") + 
# #    ylim(-0.05, 100) 
# # 
# # 
# # p4
# 
# ```
# 
# ```{r save ML internal metrics, echo = FALSE, include = TRUE}
# # output training pt summary (brake down ratio of training points), name of training points
# 
# trDat_sum <- trDat_sum %>%
#   dplyr::mutate(total_pts = sum(freq), 
#                 samp_var = var(prop),
#                 bgc = ifelse(str_detect(mname, "_bgc"), gsub("_[[:digit:]].*","", target), "all"),
#                 training_pt_type = basename(indata)) %>%
#   dplyr::mutate(training_pt_type = gsub("_pts_att.gpkg", "", training_pt_type))
# 
# ml_results = final_metrics %>%
#   dplyr::select(c(.metric, .estimate)) %>%
#   pivot_wider(names_from = .metric, values_from = .estimate) 
#    
# trDat_sum <- trDat_sum %>%
#   cbind(ml_results) 
# 
# save(trDat_sum, file =  paste(paste0('.', outDir), "model_summary.RData", sep = "/"))
# save(final_metrics, file =  paste(paste0('.', outDir), "ml_internal_metrics.RData", sep = "/"))
# 
# ```
# 
# ```{r save outputs for later comparisons, echo= FALSE, eval = TRUE}
# # add the metrics for mapping 
# bsRes_long <- bsRes %>%
#   mutate(training_pt_type = basename(indata),
#          model_name = mrep) %>%
#   dplyr::mutate(training_pt_type = gsub("_pts_att.gpkg", "", training_pt_type))
# 
# bsRes_all <- bsRes_all %>%
#    mutate(training_pt_type = basename(indata),
#          model_name = mrep) %>%
#   dplyr::mutate(training_pt_type = gsub("_pts_att.gpkg", "", training_pt_type))
# 
# 
# save(bsRes_long, file =  paste(paste0('.', outDir), "map_details.RData", sep = "/"))
# save(bsRes_all, file =  paste(paste0('.', outDir), "map_summary.RData", sep = "/"))


```
  
  
# References: 
- https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c

* [accuracy](https://yardstick.tidymodels.org/reference/accuracy.html) 
* [roc_auc](https://yardstick.tidymodels.org/reference/roc_auc)
* [sensitivity](https://yardstick.tidymodels.org/reference/sens.html) 
* [specificity](https://yardstick.tidymodels.org/reference/spec.html)
* [positive predictive value (ppv)](https://yardstick.tidymodels.org/reference/ppv.html)  
  
  