---
title: "Split Forest and Non-forest "
Script author: "Gen Perkins"
date: "22/10/2021"
output:
  html_document: default
  pdf_document: default
  word_document: default
---
```{r global_options, include=FALSE }
require(knitr)

```

This script will be used to test and compare methods for 

```{r setup, include=FALSE}

library(data.table)
library(scales)
library(caret)
library(sf)
library(ranger)
library(tidyverse)
library(fasterize)
library(stringr)
library(dplyr)
library(raster)
library(readxl)
library(foreach)
library(tidymodels)
library(themis)
library(vip)
library(stringi)
library(R.utils)

#devtools::install_github("tidymodels/tune")
#devtools::install_github("tidymodels/parsnip")

```

# Introduction

This script uses training point samples collected from Stage 1 transects and other point data collected.

This is by boot strapping the transects where 90% are use to provide training points to the build a map, and 10% are withheld and used as AA transects. 
This analytical approach is then used to guide improvements to the map.

1. Load data prepared in [04a script](https://github.com/bcgov-c/BEC_DevExchange_Work/blob/master/04a_TrainingPt_TransectImport%26Clean.Rmd). 
2. Split dataset into training and testing sets
3. Run the model and record model performance metrics
4. Best model is selected to create a map and associated entropy layer 


Prepare directories

```{r session setup, tidy = TRUE, warning=FALSE}

AOI <- "Deception"
#AOI <- "BoundaryTSA"

# set up file structure
AOI_dir <- file.path(".", paste0(AOI,"_AOI"))
cov_dir <- file.path(AOI_dir, "1_map_inputs", "covariates")
shapes_dir <- file.path(AOI_dir, "0_raw_inputs", "base_layers")
input_pnts_dir <- file.path(AOI_dir, "1_map_inputs", "trainingData")
out_dir <- file.path(AOI_dir, "3_maps_analysis","models")

# read in temp functions
source(here::here('_functions', 'model_gen_tidy.R'))
source(here::here('_functions', 'model_gen_tidy_nf.R'))
source(here::here('_functions', 'acc_metrix.R'))
source(here::here('_functions', 'doc_theme_pem.R'))
source(here::here('_functions', 'balance_recipe.R'))

# read in map and model keys
map.key  <- read.csv(file.path(AOI_dir, "_MapUnitLegend", 
                                 paste0(AOI, "_MapUnitLegend.csv")), 
                       stringsAsFactor = FALSE)

# #read in the fuzzy index
fMat <- read.csv(file.path(AOI_dir, "_MapUnitLegend", 
                                  "fuzzy_matrix_basic.csv")) %>%
  dplyr::select(c(target, Pred, fVal))
  
fMat <- data.table(fMat)

# read in model parameters 
model_param <- file.path(AOI_dir, "_MapUnitLegend", "models.xlsx")

# set up model parameters:  
mparam <- read_xlsx(model_param, "models") %>% filter(to_run == 1)
map_res <- mparam$resolution
data_res <- paste0("att_", map_res, "m")
mname <- paste0(mparam$model_name)
mrep <- mparam$model_rep

# check which catergory of model to be produced
mtype <- case_when(
  str_detect(mname, "for_nf")  ~ "forest_non_forest",
  str_detect(mname, "nf_") ~ "non_forest",
  str_detect(mname, "fore") ~ "forest"
)

# get covariates
mcov <- read_xlsx(model_param, "covariates", skip = 2) %>%
  filter(!!sym(mparam$covariates) == 1) %>%
  dplyr::select(covariate)

# get training point sets
mtpt <- read_xlsx(model_param, "training_pts", skip = 2) %>%
  filter(!!sym(mparam$training_pts) == 1) %>%
  dplyr::select(tp_code)%>%
  pull

# get the map unit level 
mmu <- read_xlsx(model_param, "map_unit", skip = 2) %>%
   filter(!!sym(mparam$map_unit) == 1) %>%
  dplyr::select(legend_column)%>%
  pull

mmu <- case_when(
  mmu == "column_mu" ~ "MapUnit", 
  mmu == "column_ss" ~ "SiteSeries",
  mmu == "column_ass" ~ "Association",
  mmu == "column_cls" ~ "Class",
  mmu == "column_grp" ~ "Group",
  mmu == "column_typ" ~ "Type"
)

# set up outfolder: 

if(!dir.exists(file.path(out_dir, mtype))){dir.create(file.path(out_dir, mtype))} 

out_dir <- file.path(out_dir, mtype, mname, mrep) 

# set up model folder: 
if(!dir.exists(out_dir)){
  dir.create(out_dir)} else { 
    print ("folder already exists for this model name - are you sure you want to overwrite the results?")}


# read in the all points dataset to enable map accuracy assessment (normally s1 or s2 points )
# 
# if(AOI == "BoundaryTSA"){
# all_points_predicted <- st_read(file.path(input_pnts_dir, data_res, "s1_transect_all_pts_att.gpkg")) %>%
#   mutate(tid = gsub("_[[:alpha:]].*","", id)) %>%
#   dplyr::select(-c('x','y', 'id')) 
# } else {
# 
# all_points_predicted <- st_read(file.path(input_pnts_dir, data_res, "s1_transect_all_pts_att.gpkg")) %>%
#   mutate(tid = gsub("_[[:alpha:]].*","",transect_id)) 
# }


```


##Create raster stack of spatial layers

```{r create raster stack of spatial variables}

res_folder <- paste0(map_res, "m")

rast_list <- list.files(file.path(cov_dir, res_folder), pattern = ".tif$", full.names = TRUE)

# filter based on covariate for model param
rast_list <- rast_list[tolower(gsub(".tif", "", basename(rast_list))) %in% tolower(mcov$covariate)]

mcols <- gsub(".tif","", tolower(basename(rast_list)))

# load bgc layer
bec_shp <- st_read(file.path(shapes_dir, "bec.gpkg"), quiet = TRUE) 

```


##Prepare training and testing datasets for the model

Using the point data generated in various forms from the 04a script, iteratively read in those points and create and save models. Model data is saved in a spreadsheet so that the spreadsheet can be filtered later to find the model with the best accuracy metrics. If the model is broken down by subzone and you have lots of different input datasets, this could take a while!

The random forest model is created using the training data set from above. Repeated cross validation is used to generate model metrics, though remember this is using the training set. We will evaluate our own metrics using the held back test set that was created above.

The model is able to be fine tuned quite nicely. Certain preprocessing parameters are included here to limit the amount of data that is used in the final model, including a correlation cutoff (remove covariates that are highly correlated), nero-zero variance detection (removes covariates that have 0 or close to 0 variance), centering and scaling of the data, and more, though these are the more common setup parameters. Having this prevents actual preprocessing to occur and allows for much more efficient processing.

Using the test dataset, the model is evaluated using the held back points. A confusion matrix is generated and then saved to the model output folder.


```{r read in datasets }
# read in sets of data 

if(length(mtpt) == 1){

  indata <- list.files(file.path(input_pnts_dir, data_res), paste0(mtpt,"_att.*.gpkg$"), full.names = TRUE)

  tpts <- st_read(indata) 
  
  infiles <- basename(indata) 
  
} else { 
   # get list of input training points for report
   infiles <- mtpt
        
   # read in first file 
   fileoi <- mtpt[1]
   indata <- list.files(file.path(input_pnts_dir, data_res), paste0(fileoi,"_att.*.gpkg$"), full.names = TRUE)
   tpts <- st_read(indata, quiet = TRUE) 
   
   cols_to_keep <- names(tpts)
   otherfiles <- mtpt[-1]
   
   for (filei in otherfiles ){
      #filei <- otherfiles[3]
     
     ind <- list.files(file.path(input_pnts_dir, data_res), paste0(filei,"_att.*.gpkg$"), full.names = TRUE)
     if(length(ind>1)) {
       ind <- ind[grep(paste0("^",filei,"_att.gpkg$"), basename(ind))]
     }
      file_read <- st_read(ind, quiet = TRUE) 
      file_names <- names(file_read)
      
      file_read <- file_read[,file_names %in% cols_to_keep]
      
      tpts <- bind_rows(tpts, file_read) 
     
   }
 
 }   
  
#  MU_count <- tpts %>% dplyr::count(mapunit1)
table(tpts$mapunit1)

# match to the key and filter for forest and non_forest points

subzones <- unique(bec_shp$MAP_LABEL)

tpts  <- tpts %>%
  cbind(st_coordinates(.)) %>%
  mutate(fnf = ifelse(str_detect(tpts$mapunit1, "_"), "forest", "non_forest")) %>%
 # mutate(fnf = ifelse(grepl(paste0('\\b', "_", '\\b', collapse = "|"), mapunit1), "forest","non_forest")) %>%
  st_join(st_transform(bec_shp[, "BGC_LABEL"], st_crs(.)), join = st_nearest_feature) %>%
  st_drop_geometry() %>% 
  dplyr::select(fnf, everything()) %>% 
  dplyr::rename(bgc_cat = BGC_LABEL) %>% 
  rename_all(.funs = tolower) %>% 
  droplevels()



# option 1: running non forest model only


# # filter for forest or non_forest points as required
# if(mtype %in% c("forest", "non_forest")) {
#    tpts <- tpts %>% filter(fnf == mtype)
# } 

# if secondary call is missing 
#if(unique(grepl("mapunit2", colnames(tpts))) == FALSE) {
#   tpts <- tpts %>%
#     dplyr::mutate(mapunit2 = NA)
#} else {
#  tpts <- tpts
#}

 tpts <- tpts %>%
    dplyr::mutate(target = as.factor(mapunit1),
                          target2 = as.factor(mapunit2))

if(mtype == "forest"){

  tpts <- tpts %>%
    dplyr::select(target, target2, tid, slice, bgc_cat, x, y, any_of(mcols))

  } else {
  
  tpts <- tpts %>%
    dplyr::select(target, target2, bgc_cat, x, y, any_of(mcols))
  
}

# ##select column of interest and match to key 
tt = tpts
 
tpts = tt
 if (!mmu == "MapUnit"){
#    
#   # select the target column using the mapkey if needed: 
    map.key.sub <- map.key %>%
      dplyr::select(!!sym(mmu), MapUnit) %>%
      dplyr::mutate(target3 = !!sym(mmu)) %>%
      dplyr::select(-!!sym(mmu)) %>%
      distinct()
  
     tpts <- tpts %>% left_join(map.key.sub, by = c("target" = "MapUnit")) %>%
      dplyr::select(-target, -target2, -bgc_cat) %>%
      dplyr::rename(target = target3) %>%
      filter(!is.na(target)) %>%
      dplyr::select(target, everything())
  }

# filter columns of interest 

  mpts <- tpts %>%
    dplyr::select(c(-x, -y))
  
  # OPTION: filter for pure calls only
#  mpts <- mpts %>%
#    dplyr::filter(is.na(target2))
  
  # OPTION: filter groups less than 20 pts
  #MU_count <- mpts %>% dplyr::count(target) %>% filter(n > 20) 
  
  mpts <- mpts %>% #filter(target %in% MU_count$target)  %>%
    droplevels() 
  
  
  
  
  
  
  
  
# option 2: Run the split between forest and non-forest 
  
tpts <- tpts %>%
    dplyr::mutate(target = as.factor(fnf))

tpts <- tpts %>%
    dplyr::select(target, any_of(mcols)) %>%
  filter(!is.na(target))

mpts = tpts

```


Run the non_forest models 

```{r}

if(str_detect(mname, 'nf_')) {

  # filter for > 20 points 
  MU_count <- mpts %>% dplyr::count(target) %>% filter(n > 20) 
  mpts <- mpts %>% filter(target %in% MU_count$target)  %>%
    filter(target != "") %>%
    droplevels() 
  
  # check covars
 # covars <- mpts %>% dplyr::select(-c("target" )) 
 # covars <- covars[complete.cases(covars[ ,1:length(covars)]),]
 
  ###############################################################

# run the function using tidy models

  model_gen_tidy_nf(trDat = mpts,
              target = "target",
              outDir = out_dir,
              mname = mname,
              infiles = infiles,
              mmu = mmu)
            
#   
} 

```


 